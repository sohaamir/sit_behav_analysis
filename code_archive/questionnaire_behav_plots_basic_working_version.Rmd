---
title: "questionnaire_behav_plots"
output: html_document
---

# Description of this script

This script runs the linear mixed-effects models as part of the behavioural analysis for questionnaire scores. It runs and plots the questionnaire equivalent of Fig 1D-I and Supp Fig 2A-C from Zhang & Glascher.

The general pipeline for these analyses is as follows:

1. Run linear mixed-effects models for each questionnaire measure:

Predict dependent variable, compare models with different random effects structures
Select winning model using AIC

2. Analyze and visualize significant relationships:

Create plots showing subject-level effects where significant, generating median-split visualizations to illustrate interactions

## Set-up and installation

```{r setup, include=FALSE, message=FALSE, cache=TRUE}
# Install packages only if they are not already installed
required_packages <- c("ggplot2", "tidyverse", "ggpubr", "rstatix", "ez", "lme4", "lmerTest", "car", "emmeans", "MuMIn", "psych", "interactions", "effects", "here", "lm.beta", "effectsize")
install_if_missing <- required_packages[!required_packages %in% installed.packages()]
if (length(install_if_missing) > 0) {
  install.packages(install_if_missing, quietly = TRUE)
}

# Load libraries
library(ggplot2)
library(tidyverse)
library(ggpubr)
library(rstatix)
library(ez)
library(lme4)
library(car)
library(readr)
library(lmerTest)
library(emmeans)
library(MuMIn)
library(corrplot)
library(reshape2)
library(psych)
library(interactions)
library(effects)
library(here)
library(conflicted)
library(lm.beta)
library(effectsize)

conflict_prefer("select", "dplyr")
conflict_prefer("filter", "dplyr")
conflicts_prefer(lmerTest::lmer)
conflicts_prefer(effectsize::eta_squared)
```

## Basic correlation matrix of questionnaires

Plot a correlation matrix of questionnaire scores

```{r}
# Read and process data
merged_data <- read_csv(here("data", "preprocessed", "merged_test_data.csv"), show_col_types = FALSE)

# Create correlation matrix dataframe with one row per participant
questionnaire_data <- merged_data %>%
  group_by(participant.id_in_session) %>%
  slice(1) %>%  # Take first instance of each participant
  select(ssms, dass, lsas, srp_sf, ami, aq_10) %>%
  ungroup()

# Calculate correlation matrix
cor_matrix <- cor(questionnaire_data, method = "pearson")

# If you want p-values for the correlations
cor_test <- psych::corr.test(questionnaire_data)

# Visualize using corrplot
corrplot(cor_matrix, 
         method = "color",
         type = "upper",
         addCoef.col = "black",
         number.cex = 0.7,
         tl.col = "black",
         tl.srt = 45,
         diag = FALSE)
```

## Choice switch probability and bet difference by group consensus

Run linear-mixed effect models for choice switch probability by group consensus

```{r}
# Read and process data
merged_data <- read_csv(here("data", "preprocessed", "merged_test_data.csv"), show_col_types = FALSE)

# Initial data checks and summaries
print("Original data questionnaire summary:")
summary(merged_data[c("ssms", "dass", "lsas", "srp_sf", "ami", "aq_10", "age")])

# Data processing with unbalanced consensus design
switch_probabilities <- merged_data %>%
  group_by(participant.id_in_session) %>%
  mutate(
    consensus_group = case_when(
      player.choice1_with == 0.5 ~ "2:2_against",
      player.choice1_with == 0 ~ "4:0_against",
      player.choice1_with == 0.25 ~ "3:1_against",
      player.choice1_with == 0.75 ~ "3:1_with",
      player.choice1_with == 1 ~ "4:0_with"
    ),
    consensus_level = case_when(
      player.choice1_with %in% c(0, 1) ~ "4:0",
      player.choice1_with %in% c(0.25, 0.75) ~ "3:1",
      player.choice1_with == 0.5 ~ "2:2"
    ),
    direction = if_else(player.choice1_with >= 0.5, "With group", "Against group")
  ) %>%
  group_by(participant.id_in_session, consensus_group, direction, gender) %>%
  summarise(
    switch_prob = mean(player.switch_vs_stay) * 100,
    .groups = 'drop'
  ) %>%
  left_join(
    merged_data %>%
      group_by(participant.id_in_session) %>%
      slice(1) %>%
      dplyr::select(participant.id_in_session, ssms, dass, lsas, srp_sf, ami, aq_10, age),
    by = "participant.id_in_session"
  )

# Create alternative dataset with 2:2 grouped in "with" condition
switch_probabilities_alt <- switch_probabilities %>%
  mutate(consensus_group = case_when(
    consensus_group == "2:2_against" ~ "2:2_with",
    TRUE ~ as.character(consensus_group)
  ))

# Scale variables and convert factors
switch_probabilities <- switch_probabilities %>%
  mutate(
    across(c(ssms, dass, lsas, srp_sf, ami, aq_10, age), scale),
    gender = factor(gender),
    consensus_group = factor(consensus_group, 
                           levels = c("2:2_against", "3:1_against", "4:0_against",
                                    "3:1_with", "4:0_with"))
  )

switch_probabilities_alt <- switch_probabilities_alt %>%
  mutate(
    across(c(ssms, dass, lsas, srp_sf, ami, aq_10, age), scale),
    gender = factor(gender),
    consensus_group = factor(consensus_group, 
                           levels = c("3:1_against", "4:0_against",
                                    "2:2_with", "3:1_with", "4:0_with"))
  )

# Modified diagnostic checking function
check_model_diagnostics <- function(model) {
  conv_check <- model@optinfo$conv$opt
  diagnostics <- list(
    convergence = conv_check,
    resid_mean = mean(residuals(model)),
    resid_sd = sd(residuals(model)),
    resid_normality = shapiro.test(residuals(model))$p.value
  )
  return(diagnostics)
}

# Modified model running function for unbalanced design
run_questionnaire_models <- function(quest_var) {
  print(paste("\nRunning models for:", quest_var))
  
  model1 <- lmer(as.formula(paste("switch_prob ~ consensus_group *", quest_var, 
                                 "+ age + (1|participant.id_in_session)")),
                data = switch_probabilities,
                control = lmerControl(optimizer = "bobyqa"))
  
  model2 <- lmer(as.formula(paste("switch_prob ~ consensus_group *", quest_var,
                                 "+ age + (1|participant.id_in_session) + (1|gender)")),
                data = switch_probabilities,
                control = lmerControl(optimizer = "bobyqa"))
  
  models_list <- list(model1, model2)
  aic_values <- sapply(models_list, AIC)
  winning_model <- models_list[[which.min(aic_values)]]
  
  alt_model <- lmer(as.formula(paste("switch_prob ~ consensus_group *", quest_var, 
                                    "+ age + (1|participant.id_in_session)")),
                   data = switch_probabilities_alt,
                   control = lmerControl(optimizer = "bobyqa"))
  
  # Get p-values using car::Anova
  model_anova <- car::Anova(winning_model, type = 2)
  interaction_term <- grep(paste0("consensus_group:", quest_var), rownames(model_anova), value = TRUE)
  interaction_p <- model_anova[interaction_term, "Pr(>Chisq)"]  # Changed from "Pr(>F)"
  
  alt_anova <- car::Anova(alt_model, type = 2)
  alt_interaction_term <- grep(paste0("consensus_group:", quest_var), rownames(alt_anova), value = TRUE)
  alt_p <- alt_anova[alt_interaction_term, "Pr(>Chisq)"]  # Changed from "Pr(>F)"
  
  diagnostics <- check_model_diagnostics(winning_model)
  
  # Return results
  return(tibble::tibble(
    questionnaire = quest_var,
    p_value = as.numeric(interaction_p),
    alt_p_value = as.numeric(alt_p),
    aic = min(aic_values),
    resid_normality = diagnostics$resid_normality,
    model_converged = !is.null(diagnostics$convergence),
    winning_model = list(winning_model)  # Store the winning model
  ))
}

# Custom theme
theme_custom <- theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "right",
    plot.title = element_text(size = 12, face = "bold"),
    plot.subtitle = element_text(size = 10),
    axis.title = element_text(size = 10),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9)
  )

# Continuous relationship plotting function
plot_continuous_relationship <- function(var, data, model_results) {
  data <- data %>%
    mutate(
      consensus_level = case_when(
        str_detect(consensus_group, "4:0") ~ "4:0",
        str_detect(consensus_group, "3:1") ~ "3:1",
        str_detect(consensus_group, "2:2") ~ "2:2"
      ),
      consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
    )
  
  effects <- data %>%
    group_by(direction, consensus_level) %>%
    summarise(
      correlation = cor(.data[[var]], switch_prob),
      .groups = 'drop'
    )
  
  p_val <- model_results$p_value[model_results$questionnaire == var]
  alt_p_val <- model_results$alt_p_value[model_results$questionnaire == var]
  
  subtitle <- if(!is.na(p_val) && !is.na(alt_p_val)) {
    paste("Primary analysis p =", format.pval(p_val, digits = 3),
          "\nRobustness check p =", format.pval(alt_p_val, digits = 3))
  } else {
    "p-values not available"
  }
  
  p <- ggplot(data, 
              aes(x = .data[[var]], 
                  y = switch_prob,
                  color = direction)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", formula = y ~ x) +
    facet_wrap(~consensus_level) +
    labs(x = var,
         y = "Choice switch probability (%)",
         title = paste("Relationship between", var, "and switch probability"),
         subtitle = subtitle,
         caption = paste("Effect sizes (r) range:", 
                        round(min(effects$correlation), 3), "to",
                        round(max(effects$correlation), 3))) +
    scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
    theme_custom
  
  print(p)
}

# Simple slopes analysis function
analyze_simple_slopes <- function(model, questionnaire_var) {
  # Create probe points for the questionnaire variable
  probe_points <- switch_probabilities %>%
    summarise(
      low = mean(!!sym(questionnaire_var)) - sd(!!sym(questionnaire_var)),
      mean = mean(!!sym(questionnaire_var)),
      high = mean(!!sym(questionnaire_var)) + sd(!!sym(questionnaire_var))
    )
  
  # Get predicted values at each probe point
  pred_data <- expand.grid(
    consensus_group = levels(switch_probabilities$consensus_group),
    quest_score = c(probe_points$low, probe_points$mean, probe_points$high)
  ) %>%
    mutate(
      quest_level = rep(c("-1 SD", "Mean", "+1 SD"), 
                       each = length(levels(switch_probabilities$consensus_group)))
    )
  
  # Add other necessary variables at their means/reference levels
  pred_data$age <- 0  # since age is scaled, 0 is the mean
  
  # Create column with questionnaire variable name
  pred_data[[questionnaire_var]] <- pred_data$quest_score
  
  # Get predictions
  pred_data$predicted <- predict(model, newdata = pred_data, re.form = NA)
  
  # Create plot
  p <- ggplot(pred_data, 
              aes(x = consensus_group, 
                  y = predicted, 
                  color = quest_level, 
                  group = quest_level)) +
    geom_line() +
    geom_point(size = 3) +
    labs(title = paste("Simple slopes analysis for", questionnaire_var),
         x = "Consensus Group",
         y = "Predicted Switch Probability (%)",
         color = paste(questionnaire_var, "Level")) +
    scale_color_manual(values = c("red", "purple", "blue")) +
    theme_custom
  
  # Get model summary for slopes
  model_summary <- summary(model)
  
  return(list(
    predictions = pred_data,
    plot = p,
    model_summary = model_summary
  ))
}

# Run main analyses
questionnaire_vars <- c("ssms", "dass", "lsas", "srp_sf", "ami", "aq_10")
model_results <- purrr::map_df(questionnaire_vars, run_questionnaire_models)

# Add FDR-adjusted p-values after collecting all results
model_results$p_adjusted <- p.adjust(model_results$p_value, method = "fdr")

# Print results
print("Model results for each questionnaire:")
print(model_results %>% 
        dplyr::select(questionnaire, p_value, alt_p_value, p_adjusted) %>%
        mutate(across(where(is.numeric), ~round(., 3))))

# Create continuous relationship plots
for(var in questionnaire_vars) {
  plot_continuous_relationship(var, switch_probabilities, model_results)
}

# Create median-split plots and run simple slopes for significant results
significant_analyses <- model_results %>%
  filter(p_adjusted < 0.05) %>%
  pull(questionnaire)

if(length(significant_analyses) > 0) {
  for(var in significant_analyses) {
    # Create median-split plot
    summary_data <- switch_probabilities %>%
      mutate(
        consensus_level = case_when(
          str_detect(consensus_group, "4:0") ~ "4:0",
          str_detect(consensus_group, "3:1") ~ "3:1",
          str_detect(consensus_group, "2:2") ~ "2:2"
        ),
        consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0")),
        quest_group = ifelse(.data[[var]] > median(.data[[var]]), "High", "Low")
      ) %>%
      group_by(consensus_level, direction, quest_group) %>%
      summarise(
        mean_switch = mean(switch_prob),
        se = sd(switch_prob) / sqrt(n()),
        .groups = 'drop'
      )
    
    # Get p-value from model_results
    p_val <- model_results$p_value[model_results$questionnaire == var]
    
    p_median <- ggplot(summary_data, 
                      aes(x = consensus_level, 
                          y = mean_switch, 
                          color = direction,
                          linetype = quest_group,
                          group = interaction(direction, quest_group))) +
      geom_line() +
      geom_point() +
      geom_errorbar(aes(ymin = mean_switch - se, 
                        ymax = mean_switch + se), 
                    width = 0.2) +
      scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
      labs(x = "Group consensus",
           y = "Choice switch probability (%)",
           title = paste("Effect of", var, "(Median-split)"),
           subtitle = if(!is.na(p_val)) {
             paste("p =", format.pval(p_val, digits = 3))
           } else {
             "p-value not available"
           }) +
      theme_custom
    
    print(p_median)
    
    # Create and fit the model directly here instead of using run_questionnaire_models
    model <- lmer(as.formula(paste("switch_prob ~ consensus_group *", var, 
                                  "+ age + (1|participant.id_in_session)")),
                 data = switch_probabilities,
                 control = lmerControl(optimizer = "bobyqa"))
    
    slopes_analysis <- analyze_simple_slopes(model, var)
    
    cat("\nSimple slopes analysis for", var, ":\n")
    print(slopes_analysis$model_summary)
    print(slopes_analysis$plot)
  }
} else {
  print("No significant results to analyze with simple slopes")
}

################## MODERATION FOR SIGNIFICANT RESULTS ONLY ###################

# Define subscale mapping
subscale_mapping <- list(
  lsas = c("lsas_p", "lsas_s"),
  dass = c("dass_a", "dass_d", "dass_s"),
  ssms = c("ssms_cd", "ssms_ia"),
  srp_sf = c("srp_sf_ipm", "srp_sf_ca", "srp_sf_els", "srp_sf_ct"),
  ami = c("ami_es", "ami_sm", "ami_ba"),
  aq_10 = NULL
)

# First extract and scale the subscale data at participant level
subscale_data <- merged_data %>%
  group_by(participant.id_in_session) %>%
  slice(1) %>%
  dplyr::select(participant.id_in_session,
         lsas_p, lsas_s,
         dass_a, dass_d, dass_s,
         ssms_cd, ssms_ia,
         srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
         ami_es, ami_sm, ami_ba) 

# Now scale the columns separately
subscale_data_scaled <- subscale_data %>%
  ungroup() %>%
  mutate(across(-participant.id_in_session, ~as.vector(scale(.x))))

# Define function for moderation analysis
run_moderation_analysis <- function(data, scale_name, subscales = NULL) {
  # Function for checking moderation assumptions
  check_moderation_assumptions <- function(model) {
    list(
      vif = car::vif(model),
      normality = shapiro.test(residuals(model))$p.value,
      convergence = model@optinfo$conv$opt
    )
  }
  
  # First run moderation for main scale
  main_model <- lmer(switch_prob ~ consensus_group * scale_name + 
                    age + (1|participant.id_in_session) + 
                    (0 + scale_name|participant.id_in_session),
                    data = data,
                    control = lmerControl(optimizer = "bobyqa"))
  
  # Get R-squared values and diagnostics
  main_r2 <- MuMIn::r.squaredGLMM(main_model)
  main_diagnostics <- check_moderation_assumptions(main_model)
  
  results <- list(main = list(
    scale = scale_name,
    model = main_model,
    summary = summary(main_model),
    r2_marginal = main_r2[1],
    r2_conditional = main_r2[2],
    diagnostics = main_diagnostics
  ))
  
  # Then run for subscales if they exist
  if (!is.null(subscales)) {
    subscale_results <- map(subscales, function(subscale) {
      if(subscale %in% names(data)) {  # Only run if subscale exists in data
        sub_model <- lmer(as.formula(paste(
          "switch_prob ~ consensus_group *", subscale,
          "+ age + (1|participant.id_in_session) + (0 +", subscale, "|participant.id_in_session)"
        )),
        data = data,
        control = lmerControl(optimizer = "bobyqa"))
        
        sub_r2 <- MuMIn::r.squaredGLMM(sub_model)
        sub_diagnostics <- check_moderation_assumptions(sub_model)
        
        list(
          scale = subscale,
          model = sub_model,
          summary = summary(sub_model),
          r2_marginal = sub_r2[1],
          r2_conditional = sub_r2[2],
          diagnostics = sub_diagnostics
        )
      } else {
        NULL
      }
    })
    results$subscales <- compact(subscale_results)  # Remove NULL results
  }
  
  return(results)
}

# Join scaled subscales to switch_probabilities
switch_probabilities_with_subscales <- switch_probabilities %>%
  left_join(subscale_data_scaled, by = "participant.id_in_session")

# Then run the moderation analysis
if(length(significant_analyses) > 0) {
  moderation_results <- map(significant_analyses, function(scale) {
    subscales <- subscale_mapping[[scale]]
    
    # Prepare data with scale and subscales
    mod_data <- switch_probabilities_with_subscales %>%
      rename(scale_name = !!scale)
    
    run_moderation_analysis(mod_data, "scale_name", subscales)
  })
  
  # Print results and create visualization
  for(i in seq_along(moderation_results)) {
    scale_name <- significant_analyses[i]
    results <- moderation_results[[i]]
    
    cat("\nModeration Analysis Results for", scale_name, "\n")
    cat("Main Scale Results:\n")
    print(car::Anova(results$main$model, type = 2))
    cat("\nMain Scale R-squared:\n")
    cat("Marginal R2:", round(results$main$r2_marginal, 3), 
        "\nConditional R2:", round(results$main$r2_conditional, 3), "\n")
    cat("\nDiagnostics:\n")
    print(results$main$diagnostics)
    
    if(!is.null(results$subscales)) {
      cat("\nSubscale Results:\n")
      for(sub_result in results$subscales) {
        cat("\nSubscale:", sub_result$scale, "\n")
        print(car::Anova(sub_result$model, type = 2))
        cat("\nSubscale R-squared:\n")
        cat("Marginal R2:", round(sub_result$r2_marginal, 3),
            "\nConditional R2:", round(sub_result$r2_conditional, 3), "\n")
        cat("\nDiagnostics:\n")
        print(sub_result$diagnostics)
      }
    }
    
    # Create visualization for significant moderations
    plot_moderation <- function(model, title) {
      # Create a data frame for predictions
      new_data <- expand.grid(
        consensus_group = unique(model@frame$consensus_group),
        scale_name = c(-1, 0, 1),
        age = 0  # mean of scaled age
      )
      
      # Get predictions
      new_data$predicted <- predict(model, newdata = new_data, re.form = NA)
      
      # Create plot
      ggplot(new_data, 
             aes(x = consensus_group, 
                 y = predicted, 
                 color = as.factor(scale_name),
                 group = scale_name)) +
        geom_line() +
        geom_point() +
        scale_color_manual(values = c("red", "purple", "blue"),
                          name = "DASS Score",
                          labels = c("-1 SD", "Mean", "+1 SD")) +
        theme_custom +
        labs(title = title,
             x = "Consensus Group",
             y = "Predicted Switch Probability (%)")
    }
    
    print(plot_moderation(results$main$model, 
                         paste("Moderation Effect of", scale_name)))
  }
} else {
  print("No significant results to analyze with moderation")
}

################## SAVE STATISTICAL RESULTS ###################

# Create a function to capture and format results
format_results <- function(model_results, moderation_results, significant_analyses) {
  # Initialize output text
  output_text <- "STATISTICAL ANALYSIS RESULTS\n\n"
  
  # Add timestamp
  output_text <- paste0(output_text, "Analysis run on: ", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), "\n\n")
  
  # Main analysis results
  output_text <- paste0(output_text, "PRIMARY ANALYSIS RESULTS:\n",
                       "================================\n")
  for(i in 1:nrow(model_results)) {
    output_text <- paste0(output_text,
                         "\nQuestionnaire: ", model_results$questionnaire[i], "\n",
                         "p-value: ", format.pval(model_results$p_value[i], digits = 3), "\n",
                         "FDR-adjusted p-value: ", format.pval(model_results$p_adjusted[i], digits = 3), "\n",
                         "Robustness check p-value: ", format.pval(model_results$alt_p_value[i], digits = 3), "\n",
                         "AIC: ", round(model_results$aic[i], 2), "\n",
                         "Residual normality (Shapiro p-value): ", format.pval(model_results$resid_normality[i], digits = 3), "\n",
                         "Model converged: ", model_results$model_converged[i], "\n")
  }
  
  # Moderation analysis results (if any)
  if(length(significant_analyses) > 0) {
    output_text <- paste0(output_text, "\n\nMODERATION ANALYSIS RESULTS:\n",
                         "================================\n")
    
    for(i in seq_along(moderation_results)) {
      scale_name <- significant_analyses[i]
      results <- moderation_results[[i]]
      
      # Main scale results
      output_text <- paste0(output_text,
                           "\nScale: ", scale_name, "\n",
                           "----------------\n",
                           "Main Scale Results:\n",
                           capture.output(car::Anova(results$main$model, type = 2)), "\n",
                           "\nR-squared values:\n",
                           "Marginal R2: ", round(results$main$r2_marginal, 3), "\n",
                           "Conditional R2: ", round(results$main$r2_conditional, 3), "\n",
                           "\nDiagnostics:\n",
                           "VIF values:\n", capture.output(results$main$diagnostics$vif), "\n",
                           "Normality test p-value: ", format.pval(results$main$diagnostics$normality, digits = 3), "\n",
                           "Model convergence: ", !is.null(results$main$diagnostics$convergence), "\n")
      
      # Subscale results
      if(!is.null(results$subscales)) {
        output_text <- paste0(output_text, "\nSubscale Results:\n")
        for(sub_result in results$subscales) {
          output_text <- paste0(output_text,
                               "\nSubscale: ", sub_result$scale, "\n",
                               capture.output(car::Anova(sub_result$model, type = 2)), "\n", 
                               "R-squared values:\n",
                               "Marginal R2: ", round(sub_result$r2_marginal, 3), "\n",
                               "Conditional R2: ", round(sub_result$r2_conditional, 3), "\n",
                               "Diagnostics:\n",
                               "VIF values:\n", capture.output(sub_result$diagnostics$vif), "\n",
                               "Normality test p-value: ", format.pval(sub_result$diagnostics$normality, digits = 3), "\n",
                               "Model convergence: ", !is.null(sub_result$diagnostics$convergence), "\n")
        }
      }
    }
  }
  
  return(output_text)
}

# Generate and save results
results_text <- format_results(model_results, moderation_results, significant_analyses)

# Create output directory if it doesn't exist
dir.create(here("output", "behav"), showWarnings = FALSE, recursive = TRUE)

# Save to file
writeLines(results_text, here("output", "behav", "ch_switch_by_group.txt"))
```

Run linear-mixed effect models for bet magnitude by group consensus, using the winning model across the whole group, and then plotting median splits for significant results

```{r}
# Read and process data
merged_data <- read_csv(here("data", "preprocessed", "merged_test_data.csv"), show_col_types = FALSE)

# Calculate bet difference and create initial data structure 
process_initial_data <- function(data, group_2_2_in_with = FALSE) {
  data %>%
    mutate(
      bet_difference = player.bet2 - player.bet1,
      consensus_group = case_when(
        player.choice1_with == 0.5 ~ ifelse(group_2_2_in_with, "2:2_with", "2:2_against"),
        player.choice1_with == 0 ~ "4:0_against",
        player.choice1_with == 0.25 ~ "3:1_against",
        player.choice1_with == 0.75 ~ "3:1_with",
        player.choice1_with == 1 ~ "4:0_with",
        TRUE ~ NA_character_
      ),
      consensus_level = case_when(
        player.choice1_with %in% c(0, 1) ~ "4:0",
        player.choice1_with %in% c(0.25, 0.75) ~ "3:1",
        player.choice1_with == 0.5 ~ "2:2",
        TRUE ~ NA_character_
      ),
      direction = if_else(player.choice1_with >= 0.5, "With group", "Against group")
    ) %>%
    group_by(participant.id_in_session, consensus_group, direction, gender) %>%
    summarise(
      mean_bet_diff = mean(bet_difference),
      .groups = 'drop'
    ) %>%
    left_join(
      data %>%
        group_by(participant.id_in_session) %>%
        slice(1) %>%
        select(participant.id_in_session, ssms, dass, lsas, srp_sf, ami, aq_10, age),
      by = "participant.id_in_session"
    ) %>%
    mutate(
      across(c(ssms, dass, lsas, srp_sf, ami, aq_10, age), scale),
      gender = factor(gender),
      consensus_group = factor(consensus_group, 
                             levels = if(!group_2_2_in_with) 
                               c("2:2_against", "3:1_against", "4:0_against", "3:1_with", "4:0_with")
                             else 
                               c("3:1_against", "4:0_against", "2:2_with", "3:1_with", "4:0_with"))
    )
}

# Process data
processed_data <- process_initial_data(merged_data, FALSE)
processed_data_alt <- process_initial_data(merged_data, TRUE)

# Custom theme
theme_custom <- theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "right",
    plot.title = element_text(size = 12, face = "bold"),
    plot.subtitle = element_text(size = 10),
    axis.title = element_text(size = 10),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9)
  )

# Enhanced model diagnostics function
check_model_diagnostics <- function(model) {
  conv_check <- model@optinfo$conv$opt
  diagnostics <- list(
    convergence = conv_check,
    resid_mean = mean(residuals(model)),
    resid_sd = sd(residuals(model)),
    resid_normality = shapiro.test(residuals(model))$p.value,
    outliers = length(boxplot.stats(residuals(model))$out)
  )
  return(diagnostics)
}

# Model running function
run_questionnaire_models <- function(quest_var) {
  print(paste("\nRunning models for:", quest_var))
  
  # Models with 2:2 in against group
  model1 <- lmer(as.formula(paste("mean_bet_diff ~ consensus_group *", quest_var, 
                                 "+ age + (1|participant.id_in_session)")),
                data = processed_data,
                control = lmerControl(optimizer = "bobyqa"))
  
  model2 <- lmer(as.formula(paste("mean_bet_diff ~ consensus_group *", quest_var,
                                 "+ age + (1|participant.id_in_session) + (1|gender)")),
                data = processed_data,
                control = lmerControl(optimizer = "bobyqa"))
  
  # Get best model
  models_list <- list(model1, model2)
  aic_values <- sapply(models_list, AIC)
  winning_model <- models_list[[which.min(aic_values)]]
  
  # Robustness check with 2:2 in with group
  alt_model <- lmer(as.formula(paste("mean_bet_diff ~ consensus_group *", quest_var, 
                                    "+ age + (1|participant.id_in_session)")),
                   data = processed_data_alt,
                   control = lmerControl(optimizer = "bobyqa"))
  
  # Get model statistics
  model_anova <- car::Anova(winning_model, type = 2)
  interaction_p <- model_anova[paste0("consensus_group:", quest_var), "Pr(>Chisq)"]
  
  alt_anova <- car::Anova(alt_model, type = 2)
  alt_p <- alt_anova[paste0("consensus_group:", quest_var), "Pr(>Chisq)"]
  
  list(
    questionnaire = quest_var,
    winning_model = winning_model,
    p_value = interaction_p,
    alt_p_value = alt_p,
    aic = min(aic_values),
    diagnostics = check_model_diagnostics(winning_model)
  )
}

# Plotting function
plot_continuous_relationship <- function(var, data, model_results) {
  data <- data %>%
    mutate(
      consensus_level = case_when(
        str_detect(consensus_group, "4:0") ~ "4:0",
        str_detect(consensus_group, "3:1") ~ "3:1",
        str_detect(consensus_group, "2:2") ~ "2:2"
      ),
      consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
    )
  
  effects <- data %>%
    group_by(direction, consensus_level) %>%
    summarise(
      correlation = cor(.data[[var]], mean_bet_diff),
      .groups = 'drop'
    )
  
  p <- ggplot(data, 
              aes(x = .data[[var]], 
                  y = mean_bet_diff,
                  color = direction)) +
    geom_point(alpha = 0.5) +
    geom_smooth(method = "lm", formula = y ~ x) +
    facet_wrap(~consensus_level) +
    labs(x = var,
         y = "Bet difference (Bet 2 - Bet 1)",
         title = paste("Relationship between", var, "and bet difference"),
         subtitle = paste("Primary analysis p =", format.pval(model_results$p_value[model_results$questionnaire == var], digits = 3),
                         "\nRobustness check p =", format.pval(model_results$alt_p_value[model_results$questionnaire == var], digits = 3)),
         caption = paste("Effect sizes (r) range:", 
                        round(min(effects$correlation), 3), "to",
                        round(max(effects$correlation), 3))) +
    scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
    theme_custom
  
  print(p)
}

# Add the simple slopes analysis function
analyze_simple_slopes <- function(model, questionnaire_var) {
  # Create probe points for the questionnaire variable
  probe_points <- processed_data %>%
    summarise(
      low = mean(!!sym(questionnaire_var)) - sd(!!sym(questionnaire_var)),
      mean = mean(!!sym(questionnaire_var)),
      high = mean(!!sym(questionnaire_var)) + sd(!!sym(questionnaire_var))
    )
  
  # Get predicted values at each probe point
  pred_data <- expand.grid(
    consensus_group = levels(processed_data$consensus_group),
    quest_score = c(probe_points$low, probe_points$mean, probe_points$high)
  ) %>%
    mutate(
      quest_level = rep(c("-1 SD", "Mean", "+1 SD"), 
                       each = length(levels(processed_data$consensus_group)))
    )
  
  # Add other necessary variables at their means/reference levels
  pred_data$age <- 0  # since age is scaled, 0 is the mean
  
  # Create column with questionnaire variable name
  pred_data[[questionnaire_var]] <- pred_data$quest_score
  
  # Get predictions
  pred_data$predicted <- predict(model, newdata = pred_data, re.form = NA)
  
  # Create plot
  p <- ggplot(pred_data, 
              aes(x = consensus_group, 
                  y = predicted, 
                  color = quest_level, 
                  group = quest_level)) +
    geom_line() +
    geom_point(size = 3) +
    labs(title = paste("Simple slopes analysis for", questionnaire_var),
         x = "Consensus Group",
         y = "Predicted Bet Difference",
         color = paste(questionnaire_var, "Level")) +
    scale_color_manual(values = c("red", "purple", "blue")) +
    theme_custom
  
  # Get model summary for slopes
  model_summary <- summary(model)
  
  return(list(
    predictions = pred_data,
    plot = p,
    model_summary = model_summary
  ))
}

# Run analyses
questionnaire_vars <- c("ssms", "dass", "lsas", "srp_sf", "ami", "aq_10")
model_results <- map_df(questionnaire_vars, function(var) {
  result <- run_questionnaire_models(var)
  tibble(
    questionnaire = result$questionnaire,
    p_value = result$p_value,
    alt_p_value = result$alt_p_value,
    p_adjusted = p.adjust(result$p_value, method = "fdr"),
    aic = result$aic,
    resid_normality = result$diagnostics$resid_normality,
    model_converged = !is.null(result$diagnostics$convergence),
    model = list(result$winning_model)
  )
})

# Print results
print("Model results for each questionnaire:")
print(model_results %>% 
        select(questionnaire, p_value, alt_p_value, p_adjusted) %>%
        mutate(across(where(is.numeric), ~round(., 3))))

# Create continuous relationship plots
for(var in questionnaire_vars) {
  plot_continuous_relationship(var, processed_data, model_results)
}

# Create median-split plots and run simple slopes for significant results
significant_vars <- model_results %>%
  filter(p_adjusted < 0.1) %>%
  pull(questionnaire)

if(length(significant_vars) > 0) {
  for(var in significant_vars) {
    # Create median-split plot
    summary_data <- processed_data %>%
      mutate(
        consensus_level = case_when(
          str_detect(consensus_group, "4:0") ~ "4:0",
          str_detect(consensus_group, "3:1") ~ "3:1",
          str_detect(consensus_group, "2:2") ~ "2:2"
        ),
        consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0")),
        quest_group = ifelse(.data[[var]] > median(.data[[var]]), "High", "Low")
      ) %>%
      group_by(consensus_level, direction, quest_group) %>%
      summarise(
        mean_bet_diff = mean(mean_bet_diff),
        se = sd(mean_bet_diff) / sqrt(n()),
        .groups = 'drop'
      )
    
    p_median <- ggplot(summary_data, 
                      aes(x = consensus_level, 
                          y = mean_bet_diff, 
                          color = direction,
                          linetype = quest_group,
                          group = interaction(direction, quest_group))) +
      geom_line() +
      geom_point() +
      geom_errorbar(aes(ymin = mean_bet_diff - se, 
                        ymax = mean_bet_diff + se), 
                    width = 0.2) +
      scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
      labs(x = "Group consensus",
           y = "Bet difference (Bet 2 - Bet 1)",
           title = paste("Effect of", var, "(Median-split)"),
           subtitle = paste("p =", format.pval(model_results$p_value[model_results$questionnaire == var], digits = 3))) +
      theme_custom
    
    print(p_median)
    
    # Run and plot simple slopes analysis
    slopes_analysis <- analyze_simple_slopes(model_results$model[[which(model_results$questionnaire == var)]], var)
    
    cat("\nSimple slopes analysis for", var, ":\n")
    print(slopes_analysis$model_summary)
    print(slopes_analysis$plot)
  }
}

################## MODERATION FOR SIGNIFICANT RESULTS ONLY ###################

# Define subscale mapping
subscale_mapping <- list(
  lsas = c("lsas_p", "lsas_s"),
  dass = c("dass_a", "dass_d", "dass_s"),
  ssms = c("ssms_cd", "ssms_ia"),
  srp_sf = c("srp_sf_ipm", "srp_sf_ca", "srp_sf_els", "srp_sf_ct"),
  ami = c("ami_es", "ami_sm", "ami_ba"),
  aq_10 = NULL
)

if(length(significant_vars) > 0) {
  # Extract and scale the subscale data at participant level
  subscale_data <- merged_data %>%
    group_by(participant.id_in_session) %>%
    slice(1) %>%
    select(participant.id_in_session,
           lsas_p, lsas_s,
           dass_a, dass_d, dass_s,
           ssms_cd, ssms_ia,
           srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
           ami_es, ami_sm, ami_ba) 
  
  # Scale the columns separately
  subscale_data_scaled <- subscale_data %>%
    ungroup() %>%
    mutate(across(-participant.id_in_session, ~as.vector(scale(.x))))
  
  # Join with processed_data
  processed_data_with_subscales <- processed_data %>%
    left_join(subscale_data_scaled, by = "participant.id_in_session")
  
  # Function for checking moderation assumptions
  check_moderation_assumptions <- function(model) {
    list(
      vif = car::vif(model),
      normality = shapiro.test(residuals(model))$p.value,
      convergence = model@optinfo$conv$opt
    )
  }
  
  # Define function for moderation analysis
  run_moderation_analysis <- function(data, scale_name, subscales = NULL) {
    # Run moderation for main scale
    main_model <- lmer(mean_bet_diff ~ consensus_group * scale_name + 
                      age + (1|participant.id_in_session) + 
                      (0 + scale_name|participant.id_in_session),
                      data = data,
                      control = lmerControl(optimizer = "bobyqa"))
    
    # Get R-squared values and diagnostics
    main_r2 <- MuMIn::r.squaredGLMM(main_model)
    main_diagnostics <- check_moderation_assumptions(main_model)
    
    results <- list(main = list(
      scale = scale_name,
      model = main_model,
      summary = summary(main_model),
      r2_marginal = main_r2[1],
      r2_conditional = main_r2[2],
      diagnostics = main_diagnostics
    ))
    
    # Run for subscales if they exist
    if (!is.null(subscales)) {
      subscale_results <- map(subscales, function(subscale) {
        if(subscale %in% names(data)) {
          sub_model <- lmer(as.formula(paste(
            "mean_bet_diff ~ consensus_group *", subscale,
            "+ age + (1|participant.id_in_session) + (0 +", subscale, "|participant.id_in_session)"
          )),
          data = data,
          control = lmerControl(optimizer = "bobyqa"))
          
          sub_r2 <- MuMIn::r.squaredGLMM(sub_model)
          sub_diagnostics <- check_moderation_assumptions(sub_model)
          
          list(
            scale = subscale,
            model = sub_model,
            summary = summary(sub_model),
            r2_marginal = sub_r2[1],
            r2_conditional = sub_r2[2],
            diagnostics = sub_diagnostics
          )
        } else {
          NULL
        }
      })
      results$subscales <- compact(subscale_results)
    }
    
    return(results)
  }
  
  # Run the moderation analysis for significant results
  moderation_results <- map(significant_vars, function(scale) {
    subscales <- subscale_mapping[[scale]]
    
    # Prepare data with scale and subscales
    mod_data <- processed_data_with_subscales %>%
      rename(scale_name = !!scale)
    
    run_moderation_analysis(mod_data, "scale_name", subscales)
  })
  
  # Function to format results
  format_results <- function(model_results, moderation_results, significant_vars) {
    # Initialize output text
    output_text <- "BET MAGNITUDE BY GROUP CONSENSUS - STATISTICAL ANALYSIS RESULTS\n\n"
    
    # Add timestamp (once)
    output_text <- paste0(output_text, "Analysis run on: ", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), "\n\n")
    
    # Main analysis results (once)
    output_text <- paste0(output_text, "PRIMARY ANALYSIS RESULTS:\n",
                         "================================\n")
    
    # Add each questionnaire result once
    for(i in 1:nrow(model_results)) {
      output_text <- paste0(output_text,
                           "\nQuestionnaire: ", model_results$questionnaire[i], "\n",
                           "p-value: ", format.pval(model_results$p_value[i], digits = 3), "\n",
                           "FDR-adjusted p-value: ", format.pval(model_results$p_adjusted[i], digits = 3), "\n",
                           "Robustness check p-value: ", format.pval(model_results$alt_p_value[i], digits = 3), "\n",
                           "AIC: ", round(model_results$aic[i], 2), "\n",
                           "Residual normality (Shapiro p-value): ", format.pval(model_results$resid_normality[i], digits = 3), "\n",
                           "Model converged: ", model_results$model_converged[i], "\n")
    }
    
    # Add moderation results once (if any)
    if(length(significant_vars) > 0) {
      output_text <- paste0(output_text, "\n\nMODERATION ANALYSIS RESULTS:\n",
                           "================================\n")
      
      # Process each significant scale once
      for(i in seq_along(significant_vars)) {
        scale_name <- significant_vars[i]
        results <- moderation_results[[i]]
        
        output_text <- paste0(output_text,
                             "\nScale: ", scale_name, "\n",
                             "----------------\n",
                             "Main Scale Results:\n",
                             capture.output(car::Anova(results$main$model, type = 2)), "\n",
                             "\nR-squared values:\n",
                             "Marginal R2: ", round(results$main$r2_marginal, 3), "\n",
                             "Conditional R2: ", round(results$main$r2_conditional, 3), "\n",
                             "\nDiagnostics:\n",
                             "VIF values:\n", paste(capture.output(results$main$diagnostics$vif), collapse = "\n"), "\n",
                             "Normality test p-value: ", format.pval(results$main$diagnostics$normality, digits = 3), "\n",
                             "Model convergence: ", !is.null(results$main$diagnostics$convergence), "\n")
        
        if(!is.null(results$subscales)) {
          output_text <- paste0(output_text, "\nSubscale Results:\n") 
          for(sub_result in results$subscales) {
            output_text <- paste0(output_text,
                                 "\nSubscale: ", sub_result$scale, "\n",
                                 paste(capture.output(car::Anova(sub_result$model, type = 2)), collapse = "\n"), "\n",
                                 "R-squared values:\n",
                                 "Marginal R2: ", round(sub_result$r2_marginal, 3), "\n",
                                 "Conditional R2: ", round(sub_result$r2_conditional, 3), "\n",
                                 "Diagnostics:\n",
                                 "VIF values:\n", paste(capture.output(sub_result$diagnostics$vif), collapse = "\n"), "\n",
                                 "Normality test p-value: ", format.pval(sub_result$diagnostics$normality, digits = 3), "\n",
                                 "Model convergence: ", !is.null(sub_result$diagnostics$convergence), "\n")
          }
        }
      }
    }
    
    return(output_text)
  }
  
  # Generate and save results
  results_text <- format_results(model_results, moderation_results, significant_vars)
  dir.create(here("output", "behav"), showWarnings = FALSE, recursive = TRUE)
  writeLines(results_text, here("output", "behav", "bet_mag_by_group.txt"))
}
```

## Choice accuracy and bet magnitude

Plot average choice accuracy and bet magnitude 

```{r}
# Read and process data
data <- read_csv(here("data", "preprocessed", "merged_test_data.csv"))

# Calculate differences per participant
participant_data <- data %>%
  group_by(participant.id_in_session) %>%
  summarise(
    choice_accuracy_diff = mean(player.choice2_accuracy - player.choice1_accuracy) * 100,
    bet_magnitude_diff = mean(player.bet2 - player.bet1)
  ) %>%
  # Join with questionnaire data (taking first instance of questionnaire scores per participant)
  left_join(
    data %>%
      group_by(participant.id_in_session) %>%
      slice(1) %>%
      dplyr::select(participant.id_in_session, ssms, dass, lsas, srp_sf, ami, aq_10, age), # Added age here
    by = "participant.id_in_session"
  ) %>%
  # Scale age and questionnaire variables
  mutate(
    across(c(ssms, dass, lsas, srp_sf, ami, aq_10, age), scale)
  )

# Function to run analysis for each questionnaire
analyze_questionnaire_effects <- function(questionnaire) {
  # For choice accuracy
  choice_model <- lm(paste("choice_accuracy_diff ~", questionnaire, "+ age"), 
                    data = participant_data)
  
  # For bet magnitude
  bet_model <- lm(paste("bet_magnitude_diff ~", questionnaire, "+ age"), 
                 data = participant_data)
  
  # Extract results and add diagnostics
  choice_stats <- summary(choice_model)
  bet_stats <- summary(bet_model)
  
  # Get standardized betas
  choice_beta <- lm.beta::lm.beta(choice_model)$standardized.coefficients[2]
  bet_beta <- lm.beta::lm.beta(bet_model)$standardized.coefficients[2]
  
  # Calculate correlations with CIs
  choice_cor <- cor.test(participant_data[[questionnaire]], 
                        participant_data$choice_accuracy_diff)
  bet_cor <- cor.test(participant_data[[questionnaire]], 
                      participant_data$bet_magnitude_diff)
  
  list(
    questionnaire = questionnaire,
    choice_p = choice_stats$coefficients[2,4],
    bet_p = bet_stats$coefficients[2,4],
    choice_r = choice_cor$estimate,
    bet_r = bet_cor$estimate,
    choice_r_ci_low = choice_cor$conf.int[1],
    choice_r_ci_high = choice_cor$conf.int[2],
    bet_r_ci_low = bet_cor$conf.int[1],
    bet_r_ci_high = bet_cor$conf.int[2],
    choice_beta = choice_beta,
    bet_beta = bet_beta,
    choice_resid_norm = shapiro.test(residuals(choice_model))$p.value,
    bet_resid_norm = shapiro.test(residuals(bet_model))$p.value
  )
}

# Run analysis for all questionnaires
questionnaires <- c("ssms", "dass", "lsas", "srp_sf", "ami", "aq_10")
results <- map_df(questionnaires, analyze_questionnaire_effects)

# Add FDR correction
results <- results %>%
  mutate(
    choice_p_adj = p.adjust(choice_p, method = "fdr"),
    bet_p_adj = p.adjust(bet_p, method = "fdr")
  )

# Modified printing section
print_results <- function(main_results, subscale_results = NULL) {
  cat("\nMAIN SCALE RESULTS:\n")
  print(main_results %>%
        select(questionnaire, 
               choice_p, choice_p_adj, choice_r, choice_beta, choice_resid_norm,
               bet_p, bet_p_adj, bet_r, bet_beta, bet_resid_norm) %>%
        mutate(across(where(is.numeric), ~round(., 3))))
  
  if(!is.null(subscale_results)) {
    cat("\nSUBSCALE RESULTS:\n")
    # Select only columns that exist
    available_cols <- names(subscale_results)
    cols_to_select <- c("parent_scale", "questionnaire",
                       intersect(c("choice_p", "choice_p_adj", "choice_r", "choice_beta", "choice_multi_beta"), available_cols),
                       intersect(c("bet_p", "bet_p_adj", "bet_r", "bet_beta", "bet_multi_beta"), available_cols))
    
    print(subscale_results %>%
          select(all_of(cols_to_select)) %>%
          mutate(across(where(is.numeric), ~round(., 3))))
  }
}

# Modified plotting function to include CIs and betas
plot_relationship <- function(questionnaire, measure = "choice", data = participant_data, results_df) {
  if(measure == "choice") {
    y_var <- "choice_accuracy_diff"
    y_lab <- "Choice accuracy difference (%)"
    fill_color <- "#1f77b4"
    r_val <- results_df$choice_r[results_df$questionnaire == questionnaire]
    p_val <- results_df$choice_p[results_df$questionnaire == questionnaire]
    beta_val <- results_df$choice_beta[results_df$questionnaire == questionnaire]
    ci_low <- results_df$choice_r_ci_low[results_df$questionnaire == questionnaire]
    ci_high <- results_df$choice_r_ci_high[results_df$questionnaire == questionnaire]
  } else {
    y_var <- "bet_magnitude_diff"
    y_lab <- "Bet magnitude difference"
    fill_color <- "#2ca02c"
    r_val <- results_df$bet_r[results_df$questionnaire == questionnaire]
    p_val <- results_df$bet_p[results_df$questionnaire == questionnaire]
    beta_val <- results_df$bet_beta[results_df$questionnaire == questionnaire]
    ci_low <- results_df$bet_r_ci_low[results_df$questionnaire == questionnaire]
    ci_high <- results_df$bet_r_ci_high[results_df$questionnaire == questionnaire]
  }
  
  p <- ggplot(data, 
              aes(x = .data[[questionnaire]], y = .data[[y_var]])) +
    geom_point() +
    geom_smooth(method = "lm", color = fill_color) +
    labs(
      x = questionnaire,
      y = y_lab,
      title = paste("Effect of", questionnaire, "on", measure),
      subtitle = sprintf(
        "r = %.3f [%.3f, %.3f], Î² = %.3f, p = %.3f",
        r_val, ci_low, ci_high, beta_val, p_val
      )
    ) +
    theme_classic()
  
  print(p)
}

# Create plots for significant relationships (change p-values accordingly: 0.5 for testing, 0.05 for actual data)
significant_choice <- results %>% 
  filter(choice_p_adj < 0.5) %>% 
  pull(questionnaire)

significant_bet <- results %>% 
  filter(bet_p_adj < 0.5) %>% 
  pull(questionnaire)

# Plot significant relationships
for(quest in significant_choice) {
  plot_relationship(quest, "choice", participant_data, results)
}

for(quest in significant_bet) {
  plot_relationship(quest, "bet", participant_data, results)
}

if(length(c(significant_choice, significant_bet)) == 0) {
  print("No significant relationships found after multiple comparison correction")
}

############ FURTHER ANALYSES FOR SUBSCALES OF SIGNFICIANT FINDINGS ABOVE ########

# Define subscale mapping
subscale_mapping <- list(
  lsas = c("lsas_p", "lsas_s"),
  dass = c("dass_a", "dass_d", "dass_s"),
  ssms = c("ssms_cd", "ssms_ia"),
  srp_sf = c("srp_sf_ipm", "srp_sf_ca", "srp_sf_els", "srp_sf_ct"),
  ami = c("ami_es", "ami_sm", "ami_ba"),
  aq_10 = NULL
)

# If there are significant relationships, analyze their subscales
if(length(c(significant_choice, significant_bet)) > 0) {
  cat("\n=== Starting Subscale Analysis ===\n")
  
  # Get unique significant questionnaires
  significant_questionnaires <- unique(c(significant_choice, significant_bet))
  cat("\nSignificant questionnaires identified:", paste(significant_questionnaires, collapse=", "), "\n")
  
  # Create subscale participant data with preserved column names
  subscale_participant_data <- data %>%
    group_by(participant.id_in_session) %>%
    summarise(
      choice_accuracy_diff = mean(player.choice2_accuracy - player.choice1_accuracy) * 100,
      bet_magnitude_diff = mean(player.bet2 - player.bet1),
      age = first(age),  # Add age here
      across(c(lsas_p, lsas_s,
               dass_a, dass_d, dass_s,
               ssms_cd, ssms_ia,
               srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
               ami_es, ami_sm, ami_ba), 
             ~first(.x), 
             .names = "{.col}")
    ) %>%
    # Scale variables
    mutate(
      across(c(age, lsas_p, lsas_s,
               dass_a, dass_d, dass_s,
               ssms_cd, ssms_ia,
               srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
               ami_es, ami_sm, ami_ba), scale)
    )
    
  # Modified function for subscale analysis
  analyze_subscale_effects <- function(subscale) {
    cat("\n    Analyzing effects for subscale:", subscale)
    
    # For choice accuracy - include age as covariate
    choice_model <- lm(paste("choice_accuracy_diff ~", subscale, "+ age"), 
                      data = subscale_participant_data)
    
    # For bet magnitude - include age as covariate
    bet_model <- lm(paste("bet_magnitude_diff ~", subscale, "+ age"), 
                   data = subscale_participant_data)
    
    # Get standardized betas
    choice_beta <- lm.beta::lm.beta(choice_model)$standardized.coefficients[2]
    bet_beta <- lm.beta::lm.beta(bet_model)$standardized.coefficients[2]
    
    # Calculate correlations with CIs
    choice_cor <- cor.test(subscale_participant_data[[subscale]], 
                          subscale_participant_data$choice_accuracy_diff)
    bet_cor <- cor.test(subscale_participant_data[[subscale]], 
                        subscale_participant_data$bet_magnitude_diff)
    
    result <- tibble(
      questionnaire = subscale,
      choice_p = summary(choice_model)$coefficients[2,4],
      bet_p = summary(bet_model)$coefficients[2,4],
      choice_r = choice_cor$estimate,
      bet_r = bet_cor$estimate,
      choice_r_ci_low = choice_cor$conf.int[1],
      choice_r_ci_high = choice_cor$conf.int[2],
      bet_r_ci_low = bet_cor$conf.int[1],
      bet_r_ci_high = bet_cor$conf.int[2],
      choice_beta = choice_beta,
      bet_beta = bet_beta,
      choice_resid_norm = shapiro.test(residuals(choice_model))$p.value,
      bet_resid_norm = shapiro.test(residuals(bet_model))$p.value
    )
    
    # Add multiple regression for subscales of same parent
    if(exists("parent_subscales") && length(parent_subscales) > 1) {
      # Create formula with all subscales
      choice_multi_formula <- paste("choice_accuracy_diff ~", 
                                  paste(parent_subscales, collapse = " + "),
                                  "+ age")
      bet_multi_formula <- paste("bet_magnitude_diff ~", 
                               paste(parent_subscales, collapse = " + "),
                               "+ age")
      
      # Run multiple regressions
      choice_multi_model <- lm(choice_multi_formula, data = subscale_participant_data)
      bet_multi_model <- lm(bet_multi_formula, data = subscale_participant_data)
      
      # Get coefficients for current subscale in multiple regression
      choice_multi_coef <- summary(choice_multi_model)$coefficients[subscale,]
      bet_multi_coef <- summary(bet_multi_model)$coefficients[subscale,]
      
      # Add to results
      result$choice_multi_p <- choice_multi_coef[4]
      result$bet_multi_p <- bet_multi_coef[4]
      result$choice_multi_beta <- lm.beta::lm.beta(choice_multi_model)$standardized.coefficients[subscale]
      result$bet_multi_beta <- lm.beta::lm.beta(bet_multi_model)$standardized.coefficients[subscale]
    }
    
    cat("\n    Results computed for", subscale, "\n")
    return(result)
  }
  
  # Modify the analysis section to:
  subscale_results <- NULL
  for(questionnaire in significant_questionnaires) {
    cat("\n\nProcessing main questionnaire:", questionnaire)
    subscales <- subscale_mapping[[questionnaire]]
    cat("\nFound subscales:", paste(subscales, collapse=", "))
    
    if(!is.null(subscales)) {
      # Create results for each subscale
      questionnaire_results <- map_df(subscales, function(subscale) {
        cat("\n  Processing subscale:", subscale, "for parent:", questionnaire)
        result <- analyze_subscale_effects(subscale) 
        result$parent_scale <- questionnaire  # Add parent scale directly to result
        cat("\n  Added parent_scale:", questionnaire, "for subscale:", subscale)
        return(result)
      })
      
      # Append to main results
      subscale_results <- bind_rows(subscale_results, questionnaire_results)
    }
  }
  
  # Print debug information about results
  cat("\n=== Subscale Results Structure ===\n")
  str(subscale_results)
  cat("\n=== First few rows of results ===\n")
  print(head(subscale_results))
  
  # Add FDR correction for subscales
  subscale_results <- subscale_results %>%
    group_by(parent_scale) %>%
    mutate(
      choice_p_adj = p.adjust(choice_p, method = "fdr"),
      bet_p_adj = p.adjust(bet_p, method = "fdr")
    ) %>%
    ungroup()
  
  # Print subscale results
  cat("\n=== Final Subscale Analysis Results ===\n")
  print(subscale_results %>%
        mutate(across(where(is.numeric), ~round(., 3))))
  
  # Plot subscale relationships
  plot_relationship_subscale <- function(questionnaire, measure = "choice", data = subscale_participant_data, results_df) {
    parent_scale <- results_df$parent_scale[results_df$questionnaire == questionnaire]
    cat("\nPlotting", questionnaire, "from parent scale:", parent_scale)
    
    if(measure == "choice") {
      y_var <- "choice_accuracy_diff"
      y_lab <- "Choice accuracy difference (%)"
      fill_color <- "#1f77b4"
      r_val <- results_df$choice_r[results_df$questionnaire == questionnaire]
      p_val <- results_df$choice_p[results_df$questionnaire == questionnaire]
      beta_val <- results_df$choice_beta[results_df$questionnaire == questionnaire]
      ci_low <- results_df$choice_r_ci_low[results_df$questionnaire == questionnaire]
      ci_high <- results_df$choice_r_ci_high[results_df$questionnaire == questionnaire]
    } else {
      y_var <- "bet_magnitude_diff"
      y_lab <- "Bet magnitude difference"
      fill_color <- "#2ca02c"
      r_val <- results_df$bet_r[results_df$questionnaire == questionnaire]
      p_val <- results_df$bet_p[results_df$questionnaire == questionnaire]
      beta_val <- results_df$bet_beta[results_df$questionnaire == questionnaire]
      ci_low <- results_df$bet_r_ci_low[results_df$questionnaire == questionnaire]
      ci_high <- results_df$bet_r_ci_high[results_df$questionnaire == questionnaire]
    }
    
    p <- ggplot(data, 
                aes(x = .data[[questionnaire]], y = .data[[y_var]])) +
      geom_point() +
      geom_smooth(method = "lm", color = fill_color) +
      labs(
        x = questionnaire,
        y = y_lab,
        title = paste("Effect of", questionnaire, "on", measure),
        subtitle = sprintf(
          "r = %.3f [%.3f, %.3f], Î² = %.3f, p = %.3f",
          r_val, ci_low, ci_high, beta_val, p_val
        )
      ) +
      theme_classic()
    
    print(p)
  }
  
  # Plot subscale relationships
  cat("\n=== Starting Plot Generation ===\n")
  for(scale in unique(subscale_results$parent_scale)) {
    cat("\nAnalyzing parent scale:", scale)
    
    # Plot for choice accuracy
    if(scale %in% significant_choice) {
      cat("\nScale is in significant_choice")
      significant_subscales <- subscale_results %>%
        filter(parent_scale == scale, choice_p_adj < 0.5) %>%
        pull(questionnaire)
      
      cat("\nSignificant subscales for choice:", paste(significant_subscales, collapse=", "))
      
      for(subscale in significant_subscales) {
        cat("\nPlotting choice subscale:", subscale)
        plot_relationship_subscale(subscale, "choice", subscale_participant_data, subscale_results)
      }
    }
    
    # Plot for bet magnitude
    if(scale %in% significant_bet) {
      cat("\nScale is in significant_bet")
      significant_subscales <- subscale_results %>%
        filter(parent_scale == scale, bet_p_adj < 0.5) %>%
        pull(questionnaire)
      
      cat("\nSignificant subscales for bet:", paste(significant_subscales, collapse=", "))
      
      for(subscale in significant_subscales) {
        cat("\nPlotting bet subscale:", subscale)
        plot_relationship_subscale(subscale, "bet", subscale_participant_data, subscale_results)
      }
    }
  }
} else {
  print("No significant main scale results to analyze at subscale level")
}

################## SAVE STATISTICAL RESULTS ###################

format_results <- function(main_results, subscale_results = NULL) {
  output_text <- "CHOICE AND BET MAGNITUDE ANALYSIS RESULTS\n\n"
  
  # Add timestamp
  output_text <- paste0(output_text, "Analysis run on: ", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), "\n\n")
  
  # Main results section
  output_text <- paste0(output_text, "PRIMARY ANALYSIS RESULTS:\n",
                       "================================\n")
  
  for(i in 1:nrow(main_results)) {
    output_text <- paste0(output_text,
                         "\nQuestionnaire: ", main_results$questionnaire[i], "\n",
                         "\nChoice Analysis:\n",
                         "p-value: ", format.pval(main_results$choice_p[i], digits = 3), "\n",
                         "FDR-adjusted p-value: ", format.pval(main_results$choice_p_adj[i], digits = 3), "\n",
                         "Correlation (r): ", round(main_results$choice_r[i], 3), 
                         " [", round(main_results$choice_r_ci_low[i], 3), ", ", 
                         round(main_results$choice_r_ci_high[i], 3), "]\n",
                         "Standardized beta: ", round(main_results$choice_beta[i], 3), "\n",
                         "Residual normality: ", format.pval(main_results$choice_resid_norm[i], digits = 3), "\n",
                         "\nBet Analysis:\n",
                         "p-value: ", format.pval(main_results$bet_p[i], digits = 3), "\n",
                         "FDR-adjusted p-value: ", format.pval(main_results$bet_p_adj[i], digits = 3), "\n",
                         "Correlation (r): ", round(main_results$bet_r[i], 3),
                         " [", round(main_results$bet_r_ci_low[i], 3), ", ", 
                         round(main_results$bet_r_ci_high[i], 3), "]\n",
                         "Standardized beta: ", round(main_results$bet_beta[i], 3), "\n",
                         "Residual normality: ", format.pval(main_results$bet_resid_norm[i], digits = 3), "\n")
  }
  
  # Subscale results section
  if(!is.null(subscale_results)) {
    output_text <- paste0(output_text, "\n\nSUBSCALE ANALYSIS RESULTS:\n",
                         "================================\n")
    
    for(scale in unique(subscale_results$parent_scale)) {
      output_text <- paste0(output_text, "\nParent Scale: ", scale, "\n")
      
      scale_results <- subscale_results %>% filter(parent_scale == scale)
      
      for(i in 1:nrow(scale_results)) {
        # Basic results that always exist
        subscale_text <- paste0("\nSubscale: ", scale_results$questionnaire[i], "\n",
                               "\nChoice Analysis:\n",
                               "p-value: ", format.pval(scale_results$choice_p[i], digits = 3), "\n",
                               "FDR-adjusted p-value: ", format.pval(scale_results$choice_p_adj[i], digits = 3), "\n",
                               "Correlation (r): ", round(scale_results$choice_r[i], 3),
                               " [", round(scale_results$choice_r_ci_low[i], 3), ", ", 
                               round(scale_results$choice_r_ci_high[i], 3), "]\n",
                               "Standardized beta: ", round(scale_results$choice_beta[i], 3), "\n",
                               "Residual normality: ", format.pval(scale_results$choice_resid_norm[i], digits = 3), "\n",
                               "\nBet Analysis:\n",
                               "p-value: ", format.pval(scale_results$bet_p[i], digits = 3), "\n",
                               "FDR-adjusted p-value: ", format.pval(scale_results$bet_p_adj[i], digits = 3), "\n",
                               "Correlation (r): ", round(scale_results$bet_r[i], 3),
                               " [", round(scale_results$bet_r_ci_low[i], 3), ", ", 
                               round(scale_results$bet_r_ci_high[i], 3), "]\n",
                               "Standardized beta: ", round(scale_results$bet_beta[i], 3), "\n",
                               "Residual normality: ", format.pval(scale_results$bet_resid_norm[i], digits = 3), "\n")
        
        # Add multiple regression results only if they exist
        if("choice_multi_beta" %in% names(scale_results)) {
          subscale_text <- paste0(subscale_text,
                                "Multiple regression beta (Choice): ", round(scale_results$choice_multi_beta[i], 3), "\n")
        }
        if("bet_multi_beta" %in% names(scale_results)) {
          subscale_text <- paste0(subscale_text,
                                "Multiple regression beta (Bet): ", round(scale_results$bet_multi_beta[i], 3), "\n")
        }
        
        output_text <- paste0(output_text, subscale_text)
      }
    }
  }
  
  return(output_text)
}

# Generate and save results
results_text <- format_results(results, subscale_results)

# Create output directory if it doesn't exist
dir.create(here("output", "behav"), showWarnings = FALSE, recursive = TRUE)

# Save to file
writeLines(results_text, here("output", "behav", "choice_and_bet_magnitude.txt"))
```

## Choice accuracy and bet magnitude and switching across trials/group consensus

Plot Choice 1 accuracy as a function of switch difference across trials (Choice 2 - Choice 1) and group consensus, questionnaire pipeline

```{r}
# Read and process data
merged_data <- read_csv(here("data", "preprocessed", "merged_test_data.csv"), show_col_types = FALSE)

# Create separate datasets for switch and stay trials
switch_data <- merged_data %>% 
 filter(choice_switch_across_trials == 1) %>%
 mutate(
   consensus_numeric = case_when(
     player.choice1_with %in% c(0, 1) ~ 2,  # 4:0
     player.choice1_with %in% c(0.25, 0.75) ~ 1,  # 3:1
     player.choice1_with == 0.5 ~ 0  # 2:2
   ),
   direction = if_else(player.choice1_with > 0.5, "With group", "Against group")
 )

stay_data <- merged_data %>%
 filter(choice_switch_across_trials == 0) %>%
 mutate(
   consensus_numeric = case_when(
     player.choice1_with %in% c(0, 1) ~ 2,  # 4:0
     player.choice1_with %in% c(0.25, 0.75) ~ 1,  # 3:1
     player.choice1_with == 0.5 ~ 0  # 2:2
   ),
   direction = if_else(player.choice1_with > 0.5, "With group", "Against group")
 )

# Modified diagnostic checking function
check_model_diagnostics <- function(model) {
 conv_check <- model@optinfo$conv$opt
 diagnostics <- list(
   convergence = conv_check,
   resid_mean = mean(residuals(model)),
   resid_sd = sd(residuals(model)),
   resid_normality = shapiro.test(residuals(model))$p.value
 )
 return(diagnostics)
}

# Check model assumptions
check_model_assumptions <- function(model) {
  # Get residuals
  resids <- residuals(model)
  fitted_vals <- fitted(model)
  
  # Test for homoscedasticity using a correlation test between
  # absolute residuals and fitted values
  homoscedasticity_test <- cor.test(abs(resids), fitted_vals)
  
  list(
    normality = shapiro.test(resids),
    homoscedasticity = list(
      correlation = homoscedasticity_test$estimate,
      p_value = homoscedasticity_test$p.value,
      interpretation = if(homoscedasticity_test$p.value < 0.05) 
        "Potential heteroscedasticity" else "No clear evidence of heteroscedasticity"
    ),
    vif = car::vif(model)
  )
}

# Modified model running function
run_questionnaire_models <- function(quest_var, data_type) {
 print(paste("\nRunning models for:", quest_var, "in", data_type, "trials"))
 
 # Select appropriate dataset
 data <- if(data_type == "switch") switch_data else stay_data
 
 # Scale questionnaire variables
 data <- data %>%
   mutate(
     across(c(ssms, dass, lsas, srp_sf, ami, aq_10, age), scale),
     gender = factor(gender),
     direction = factor(direction)
   )
 
 # Run models
 model1 <- lmer(as.formula(paste("player.choice1_accuracy ~ direction * consensus_numeric *", 
                                quest_var, "+ age + (1|participant.id_in_session)")),
               data = data,
               control = lmerControl(optimizer = "bobyqa"))
 
 model2 <- lmer(as.formula(paste("player.choice1_accuracy ~ direction * consensus_numeric *", 
                                quest_var, "+ age + (1|participant.id_in_session) + (1|gender)")),
               data = data,
               control = lmerControl(optimizer = "bobyqa"))
 
 # Get best model
 models_list <- list(model1, model2)
 aic_values <- sapply(models_list, AIC)
 winning_model <- models_list[[which.min(aic_values)]]
 
 # Get model statistics
 model_anova <- car::Anova(winning_model, type = 2)
 interaction_p <- model_anova[paste0("direction:consensus_numeric:", quest_var), "Pr(>Chisq)"]
 
 list(
   questionnaire = quest_var,
   trial_type = data_type,
   winning_model = winning_model,
   p_value = interaction_p,
   aic = min(aic_values),
   diagnostics = check_model_diagnostics(winning_model)
 )
}

# Custom theme
theme_custom <- theme_minimal() +
 theme(
   panel.grid.minor = element_blank(),
   legend.position = "right",
   plot.title = element_text(size = 12, face = "bold"),
   plot.subtitle = element_text(size = 10),
   axis.title = element_text(size = 10),
   legend.title = element_text(size = 10),
   legend.text = element_text(size = 9)
 )

# Continuous relationship plotting function
plot_continuous_relationship <- function(var, all_data, model_results) {
 # Combine data from both trial types and calculate subject-level averages
 subject_averages <- bind_rows(
   switch_data %>% mutate(trial_type = "Switch trials"),
   stay_data %>% mutate(trial_type = "Stay trials")
 ) %>%
   group_by(participant.id_in_session, direction, consensus_numeric, trial_type, .data[[var]]) %>%
   summarise(
     mean_accuracy = mean(player.choice1_accuracy) * 100,
     .groups = 'drop'
   ) %>%
   mutate(
     consensus_level = case_when(
       consensus_numeric == 2 ~ "4:0",
       consensus_numeric == 1 ~ "3:1",
       consensus_numeric == 0 ~ "2:2"
     ),
     consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
   )
 
 # Calculate correlations for effect sizes
 effects <- subject_averages %>%
   group_by(direction, consensus_level, trial_type) %>%
   summarise(
     correlation = cor(.data[[var]], mean_accuracy),
     .groups = 'drop'
   )
 
 p <- ggplot(subject_averages, 
             aes(x = .data[[var]], 
                 y = mean_accuracy,
                 color = direction)) +
   geom_point(alpha = 0.5) +
   geom_smooth(method = "lm", formula = y ~ x) +
   facet_grid(trial_type ~ consensus_level) +
   labs(x = var,
        y = "Choice 1 accuracy on t + 1 (%)",
        title = paste("Relationship between", var, "and accuracy"),
        subtitle = paste("p =", format.pval(model_results$p_value[
          model_results$questionnaire == var], digits = 3)),
        caption = paste("Effect sizes (r) range:", 
                       round(min(effects$correlation), 3), "to",
                       round(max(effects$correlation), 3))) +
   scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
   theme_custom
 
 print(p)
}

# Run analyses for both conditions
questionnaire_vars <- c("ssms", "dass", "lsas", "srp_sf", "ami", "aq_10")
trial_types <- c("switch", "stay")

# Create empty list to store results
all_results <- list()

# Simple slopes analysis function
analyze_simple_slopes <- function(model, questionnaire_var, trial_type) {
  # Get appropriate dataset
  data <- if(trial_type == "switch") switch_data else stay_data
  
  # Create probe points for the questionnaire variable
  probe_points <- data %>%
    summarise(
      low = mean(!!sym(questionnaire_var)) - sd(!!sym(questionnaire_var)),
      mean = mean(!!sym(questionnaire_var)),
      high = mean(!!sym(questionnaire_var)) + sd(!!sym(questionnaire_var))
    )
  
  # Create prediction grid
  pred_data <- expand.grid(
    direction = unique(data$direction),
    consensus_numeric = unique(data$consensus_numeric),
    quest_score = c(probe_points$low, probe_points$mean, probe_points$high)
  ) %>%
    mutate(
      quest_level = rep(c("-1 SD", "Mean", "+1 SD"), 
                       each = length(unique(data$direction)) * length(unique(data$consensus_numeric))),
      direction = factor(direction)
    )
  
  # Add other necessary variables at their means/reference levels
  pred_data$age <- 0  # since age is scaled, 0 is the mean
  
  # Create column with questionnaire variable name
  pred_data[[questionnaire_var]] <- pred_data$quest_score
  
  # Get predictions
  pred_data$predicted <- predict(model, newdata = pred_data, re.form = NA)
  
  # Convert consensus numeric to factor for plotting
  pred_data <- pred_data %>%
    mutate(
      consensus_level = case_when(
        consensus_numeric == 2 ~ "4:0",
        consensus_numeric == 1 ~ "3:1",
        consensus_numeric == 0 ~ "2:2"
      ),
      consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
    )
  
  # Create plot
  p <- ggplot(pred_data, 
              aes(x = consensus_level, 
                  y = predicted * 100,  # Convert to percentage
                  color = direction,
                  linetype = quest_level,
                  group = interaction(direction, quest_level))) +
    geom_line() +
    geom_point(size = 3) +
    labs(title = paste("Simple slopes analysis for", questionnaire_var, "in", trial_type, "trials"),
         x = "Consensus Level",
         y = "Predicted Choice 1 Accuracy (%)",
         color = "Direction",
         linetype = paste(questionnaire_var, "Level")) +
    scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
    theme_custom
  
  # Get model summary for slopes
  model_summary <- summary(model)
  
  return(list(
    predictions = pred_data,
    plot = p,
    model_summary = model_summary
  ))
}

# Run models for each combination
for(var in questionnaire_vars) {
  for(trial in trial_types) {
    result <- run_questionnaire_models(var, trial)
    all_results[[paste(var, trial, sep="_")]] <- tibble(
      questionnaire = result$questionnaire,
      trial_type = result$trial_type,
      p_value = result$p_value,
      p_adjusted = p.adjust(result$p_value, method = "fdr"),
      aic = result$aic,
      resid_normality = result$diagnostics$resid_normality,
      model_converged = !is.null(result$diagnostics$convergence)
    )
  }
}

# Combine results into a single dataframe
model_results <- bind_rows(all_results)

# Print results
print("Model results for each questionnaire and trial type:")
print(model_results %>% 
        select(questionnaire, trial_type, p_value, p_adjusted) %>%
        mutate(across(where(is.numeric), ~round(., 3))))

# Create plots for significant relationships
significant_results <- model_results %>%
  filter(p_adjusted < 0.1)

if(nrow(significant_results) > 0) {
  for(i in 1:nrow(significant_results)) {
    var <- significant_results$questionnaire[i]
    trial_type <- significant_results$trial_type[i]
    
    # Continuous relationship plot
    plot_continuous_relationship(var, merged_data, model_results)
    
    # Median-split plot
    summary_data <- bind_rows(
      switch_data %>% mutate(trial_type = "Switch trials"),
      stay_data %>% mutate(trial_type = "Stay trials")
    ) %>%
      mutate(
        consensus_level = case_when(
          consensus_numeric == 2 ~ "4:0",
          consensus_numeric == 1 ~ "3:1",
          consensus_numeric == 0 ~ "2:2"
        ),
        consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0")),
        quest_group = ifelse(.data[[var]] > median(.data[[var]]), "High", "Low")
      ) %>%
      group_by(consensus_level, direction, quest_group, trial_type) %>%
      summarise(
        mean_accuracy = mean(player.choice1_accuracy * 100),
        se = sd(player.choice1_accuracy * 100) / sqrt(n()),
        .groups = 'drop'
      )
    
    p_median <- ggplot(summary_data, 
                      aes(x = consensus_level, 
                          y = mean_accuracy, 
                          color = direction,
                          linetype = quest_group,
                          group = interaction(direction, quest_group))) +
      geom_line() +
      geom_point() +
      geom_errorbar(aes(ymin = mean_accuracy - se, 
                        ymax = mean_accuracy + se), 
                    width = 0.2) +
      facet_grid(trial_type ~ .) +
      scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
      labs(x = "Group consensus",
           y = "Choice 1 accuracy in t + 1 (%)",
           title = paste("Effect of", var, "(Median-split)"),
           subtitle = paste("p =", format.pval(significant_results$p_value[i], digits = 3))) +
      theme_custom
    
    print(p_median)
    
    # Run and plot simple slopes analysis
    result <- run_questionnaire_models(var, trial_type)
    slopes_analysis <- analyze_simple_slopes(result$winning_model, var, trial_type)
    
    cat("\nSimple slopes analysis for", var, "in", trial_type, ":\n")
    print(slopes_analysis$model_summary)
    print(slopes_analysis$plot)
  }
} else {
  print("No significant relationships found after multiple comparison correction")
}

################## MODERATION FOR SIGNIFICANT RESULTS ONLY ###################

# Define subscale mapping
subscale_mapping <- list(
  lsas = c("lsas_p", "lsas_s"),
  dass = c("dass_a", "dass_d", "dass_s"),
  ssms = c("ssms_cd", "ssms_ia"),
  srp_sf = c("srp_sf_ipm", "srp_sf_ca", "srp_sf_els", "srp_sf_ct"),
  ami = c("ami_es", "ami_sm", "ami_ba"),
  aq_10 = NULL
)

# Extract and scale the subscale data at participant level
subscale_data <- merged_data %>%
  group_by(participant.id_in_session) %>%
  slice(1) %>%
  select(participant.id_in_session,
         lsas_p, lsas_s,
         dass_a, dass_d, dass_s,
         ssms_cd, ssms_ia,
         srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
         ami_es, ami_sm, ami_ba) 

# Scale the columns separately
subscale_data_scaled <- subscale_data %>%
  ungroup() %>%
  mutate(across(-participant.id_in_session, ~as.vector(scale(.x))))

# Join scaled subscales to each dataset
switch_data_with_subscales <- switch_data %>%
  left_join(subscale_data_scaled, by = "participant.id_in_session")

stay_data_with_subscales <- stay_data %>%
  left_join(subscale_data_scaled, by = "participant.id_in_session")

calculate_effect_size <- function(model) {
  # Get variance components
  vc <- VarCorr(model)
  # Sum random effects variances
  var_random <- sum(sapply(vc, function(x) x[1]))
  # Get residual variance
  var_residual <- attr(vc, "sc")^2
  # Get variance of fixed effects
  var_fixed <- var(fitted(model))
  # Calculate marginal R2
  r2_marginal <- var_fixed / (var_fixed + var_random + var_residual)
  return(r2_marginal)
}

# Update run_moderation_analysis function to properly handle subscales
run_moderation_analysis <- function(data, scale_name, subscales = NULL, trial_type) {
  # Run base model without any scale moderation for comparison
  base_model <- lmer(player.choice1_accuracy ~ direction * consensus_numeric + 
                     age + (1|participant.id_in_session),
                     data = data,
                     control = lmerControl(optimizer = "bobyqa"))
  
  # Run moderation for main scale
  main_model <- lmer(player.choice1_accuracy ~ direction * consensus_numeric * scale_name + 
                     age + (1|participant.id_in_session),
                     data = data,
                     control = lmerControl(optimizer = "bobyqa"))
  
  # Compare models to test significance of moderation
  main_comparison <- anova(base_model, main_model)
  
  results <- list(
    main = list(
      scale = scale_name,
      base_model = base_model,
      model = main_model,
      model_comparison = main_comparison,
      summary = summary(main_model)
    )
  )
  
  # Add effect size to results
  results$main$effect_size <- calculate_effect_size(main_model)
  
  # Add after main model fitting
  results$main$assumptions <- check_model_assumptions(main_model)
  
  # Run for subscales if they exist
  if (!is.null(subscales)) {
    subscale_results <- map(subscales, function(subscale) {
      if(subscale %in% names(data)) {
        # Use the main scale model as the base for subscale comparison
        base_sub_model <- main_model
        
        # Create formula for subscale model
        sub_formula <- paste0("player.choice1_accuracy ~ direction * consensus_numeric * scale_name + ",
                           "direction * consensus_numeric * `", subscale, "` + ",
                           "scale_name * `", subscale, "` + ",
                           "age + (1|participant.id_in_session)")
        
        # Run subscale model
        sub_model <- lmer(as.formula(sub_formula),
                         data = data,
                         control = lmerControl(optimizer = "bobyqa"))
        
        # Compare models
        sub_comparison <- anova(base_sub_model, sub_model)
        
        # Add effect size
        effect_size <- calculate_effect_size(sub_model)
        
        # Add assumptions check
        assumptions <- check_model_assumptions(sub_model)
        
        list(
          scale = subscale,
          base_model = base_sub_model,
          model = sub_model,
          model_comparison = sub_comparison,
          summary = summary(sub_model),
          effect_size = effect_size,  # Add this line
          assumptions = assumptions
        )
      } else {
        NULL
      }
    })
    
    # Filter out NULL results and add to main results
    results$subscales <- compact(subscale_results)
  }
  
  # Add before return(results)
  if (!is.null(subscales)) {
    p_values <- sapply(results$subscales, function(x) x$model_comparison$`Pr(>Chisq)`[2])
    adj_p_values <- p.adjust(p_values, method = "fdr")
    for(i in seq_along(results$subscales)) {
      results$subscales[[i]]$adjusted_p <- adj_p_values[i]
    }
  }
  
  return(results)
}

# Fix analyze_moderation_effects function to properly handle variable names
analyze_moderation_effects <- function(model, scale_name) {
  
  # Extract coefficients
  coefs <- fixef(model)
  
  # Get variance-covariance matrix
  vcov_matrix <- vcov(model)
  
  # Calculate simple slopes at different levels of moderator
  moderator_levels <- c(-1, 0, 1)  # -1 SD, Mean, +1 SD
  
  # Handle backticked variable names
  scale_name_clean <- gsub("`", "", scale_name)
  
  simple_slopes <- map_dfr(moderator_levels, function(mod_level) {
    # Calculate simple slope - ensure proper coefficient names
    slope <- coefs["consensus_numeric"] + 
            coefs[paste0("consensus_numeric:", scale_name_clean)] * mod_level
    
    # Calculate standard error
    var_terms <- c("consensus_numeric", 
                   paste0("consensus_numeric:", scale_name_clean))
    cov_subset <- vcov_matrix[var_terms, var_terms]
    design_matrix <- c(1, mod_level)
    se <- sqrt(as.numeric(t(design_matrix) %*% cov_subset %*% design_matrix))
    
    tibble(
      moderator_level = mod_level,
      slope = as.numeric(slope),
      se = se,
      t_value = slope / se,
      p_value = 2 * pt(abs(slope / se), df = df.residual(model), lower.tail = FALSE)
    )
  })
  
  return(simple_slopes)
}

create_moderation_plot <- function(model, scale_name, trial_type) {
  # Get the model frame and terms
  mod_frame <- model@frame
  
  # Create base prediction grid ensuring points will be connected in order
  new_data <- expand.grid(
    direction = factor(unique(mod_frame$direction), 
                      levels = levels(mod_frame$direction)),
    consensus_numeric = sort(unique(mod_frame$consensus_numeric)),
    age = 0
  ) %>%
    arrange(direction, consensus_numeric)  # Important for line connections
  
  # Handle scale variables
  is_subscale <- !scale_name %in% c("scale_name", "dass", "ami")
  
  if(is_subscale) {
    n_conditions <- nrow(new_data)
    new_data$scale_name <- rep(0, nrow(new_data))
    new_data[[scale_name]] <- rep(c(-1, 0, 1), each = n_conditions/3)
  } else {
    n_conditions <- nrow(new_data)
    new_data$scale_name <- rep(c(-1, 0, 1), each = n_conditions/3)
  }
  
  # Make predictions
  new_data$predicted <- tryCatch({
    predict(model, newdata = new_data, re.form = NA)
  }, error = function(e) {
    message("\nDetailed prediction error: ", e$message)
    return(NULL)
  })
  
  if(is.null(new_data$predicted)) {
    warning("Could not generate predictions for plot")
    return(NULL)
  }
  
  # Format data for plotting
  plot_data <- new_data %>%
    mutate(
      consensus_level = case_when(
        consensus_numeric == 2 ~ "4:0",
        consensus_numeric == 1 ~ "3:1",
        consensus_numeric == 0 ~ "2:2"
      ),
      consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0")),
      scale_level = if(is_subscale) {
        factor(!!sym(scale_name),
               levels = c(-1, 0, 1),
               labels = c("-1 SD", "Mean", "+1 SD"))
      } else {
        factor(scale_name,
               levels = c(-1, 0, 1),
               labels = c("-1 SD", "Mean", "+1 SD"))
      }
    )
  
  # Create plot with connected lines
  p <- ggplot(plot_data, 
              aes(x = consensus_level, 
                  y = predicted * 100,
                  color = direction,
                  group = interaction(direction, scale_level))) +
    geom_line(aes(linetype = scale_level)) +
    geom_point(size = 3) +
    scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
    theme_custom +
    labs(title = paste("Moderation Effect of", scale_name, "in", trial_type),
         subtitle = "Predicted Choice 1 Accuracy by Consensus Level",
         x = "Consensus Level",
         y = "Predicted Choice 1 Accuracy (%)",
         color = "Direction",
         linetype = paste(scale_name, "Level"))
  
  print(p)
}

# Create a simple slopes function for moderation analyses
create_moderation_slopes_plot <- function(model, scale_name, trial_type) {
  mod_frame <- model@frame
  
  # Create prediction grid for continuous relationship
  new_data <- expand.grid(
    direction = factor(unique(mod_frame$direction)),
    consensus_numeric = seq(min(mod_frame$consensus_numeric), 
                          max(mod_frame$consensus_numeric), 
                          length.out = 100),
    age = 0
  )
  
  # Add scale levels
  for(level in c(-1, 0, 1)) {  # -1 SD, Mean, +1 SD
    new_data_level <- new_data
    new_data_level$scale_name <- level
    
    if(!exists("pred_data")) {
      pred_data <- new_data_level
    } else {
      pred_data <- rbind(pred_data, new_data_level)
    }
  }
  
  # Get predictions
  pred_data$predicted <- predict(model, newdata = pred_data, re.form = NA)
  
  # Create plot
  p <- ggplot(pred_data, 
              aes(x = consensus_numeric,
                  y = predicted * 100,
                  color = direction)) +
    geom_line(aes(linetype = factor(scale_name,
                                   levels = c(-1, 0, 1),
                                   labels = c("-1 SD", "Mean", "+1 SD")))) +
    scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
    theme_custom +
    labs(title = paste("Simple Slopes Analysis for", scale_name, "in", trial_type),
         subtitle = "Moderation Effect on Choice 1 Accuracy",
         x = "Consensus Level",
         y = "Predicted Choice 1 Accuracy (%)",
         color = "Direction",
         linetype = "Scale Level")
  
  print(p)
}

print("Column names in stay_data:")
print(names(stay_data))

print("\nColumn names in subscale_data_scaled:")
print(names(subscale_data_scaled))

# Let's try joining with a simpler approach first
stay_data_with_subscales <- stay_data %>%
  left_join(subscale_data_scaled)

print("\nVerifying join worked and subscales are present:")
print(names(select(stay_data_with_subscales, contains("dass"), contains("ami"))))

# Now we can proceed with the moderation analysis once we confirm the join works
if(nrow(significant_results) > 0) {
  moderation_results <- map2(significant_results$questionnaire, 
                           significant_results$trial_type,
                           function(scale, trial_type) {
    subscales <- subscale_mapping[[scale]]
    print(paste("\nAnalyzing scale:", scale))
    print(paste("With subscales:", paste(subscales, collapse=", ")))
    
    # Use joined data
    data <- stay_data_with_subscales
    
    # Check available subscales
    available_subscales <- subscales[subscales %in% names(data)]
    print(paste("Available subscales:", paste(available_subscales, collapse=", ")))
    
    # Prepare data 
    mod_data <- data %>%
      rename(scale_name = !!scale)
    
    # Run moderation analysis
    results <- run_moderation_analysis(mod_data, "scale_name", available_subscales, trial_type)
    
    # Add simple slopes analysis
    results$main$simple_slopes <- analyze_moderation_effects(results$main$model, "scale_name")
    
    if(!is.null(results$subscales)) {
      results$subscales <- map(results$subscales, function(sub_result) {
        sub_result$simple_slopes <- analyze_moderation_effects(sub_result$model, sub_result$scale)
        sub_result
      })
    }
    
    results
  })

  # Print comprehensive results
  for(i in seq_along(moderation_results)) {
    scale_name <- significant_results$questionnaire[i]
    trial_type <- significant_results$trial_type[i]
    results <- moderation_results[[i]]
    
    cat("\n\nModeration Analysis Results for", scale_name, "in", trial_type, "trials\n")
    cat("\nMain Scale Results:\n")
    print(results$main$model_comparison)
    cat("\nSimple Slopes Analysis:\n")
    print(results$main$simple_slopes)
    
    # Only create main scale plot if significant
    if(results$main$model_comparison$`Pr(>Chisq)`[2] < 0.2) {
      create_moderation_plot(results$main$model, scale_name, trial_type)
      create_moderation_slopes_plot(results$main$model, scale_name, trial_type)
    }
    
    if(!is.null(results$subscales)) {
      cat("\nSubscale Results:\n")
      for(sub_result in results$subscales) {
        cat("\nSubscale:", sub_result$scale, "\n")
        print(sub_result$model_comparison)
        cat("\nSimple Slopes Analysis:\n")
        print(sub_result$simple_slopes)
        
        # Only create subscale plot if significant
        if(sub_result$model_comparison$`Pr(>Chisq)`[2] < 0.2) {
          create_moderation_plot(sub_result$model, sub_result$scale, trial_type)
          create_moderation_slopes_plot(sub_result$model, sub_result$scale, trial_type)
        }
      }
    }
  }
} else {
  print("No significant results to analyze with moderation")
}

################## SAVE STATISTICAL RESULTS ###################

format_switch_consensus_results <- function(model_results, moderation_results = NULL) {
  output_text <- "CHOICE ACCURACY BY SWITCH AND CONSENSUS ANALYSIS RESULTS\n\n"
  
  # Add timestamp
  output_text <- paste0(output_text, "Analysis run on: ", format(Sys.time(), "%Y-%m-%d %H:%M:%S"), "\n\n")
  
  # Main results section
  output_text <- paste0(output_text, "PRIMARY ANALYSIS RESULTS:\n",
                       "================================\n")
  
  # Add main model results
  for(i in 1:nrow(model_results)) {
    output_text <- paste0(output_text,
                         "\nQuestionnaire: ", model_results$questionnaire[i], "\n",
                         "Trial type: ", model_results$trial_type[i], "\n",
                         "p-value: ", format.pval(model_results$p_value[i], digits = 3), "\n",
                         "FDR-adjusted p-value: ", format.pval(model_results$p_adjusted[i], digits = 3), "\n",
                         "AIC: ", round(model_results$aic[i], 2), "\n",
                         "Residual normality (p): ", format.pval(model_results$resid_normality[i], digits = 3), "\n",
                         "Model converged: ", model_results$model_converged[i], "\n")
  }
  
  # Moderation results section (for significant results)
  if(!is.null(moderation_results)) {
    output_text <- paste0(output_text, "\n\nMODERATION ANALYSIS RESULTS:\n",
                         "================================\n")
    
    for(i in seq_along(moderation_results)) {
      results <- moderation_results[[i]]
      scale_name <- results$main$scale
      
      # Main scale moderation results
      output_text <- paste0(output_text, 
                           "\nScale: ", scale_name, "\n",
                           "Main effect chi-square: ", round(results$main$model_comparison$Chisq[2], 3), "\n",
                           "Main effect p-value: ", format.pval(results$main$model_comparison$`Pr(>Chisq)`[2], digits = 3), "\n",
                           "Effect size (RÂ² marginal): ", round(results$main$effect_size, 3), "\n\n")
      
      # Simple slopes results
      output_text <- paste0(output_text, "Simple Slopes Analysis:\n")
      slopes_df <- results$main$simple_slopes
      for(j in 1:nrow(slopes_df)) {
        output_text <- paste0(output_text,
                             "At ", slopes_df$moderator_level[j], " SD:\n",
                             "Slope: ", round(slopes_df$slope[j], 3), "\n",
                             "SE: ", round(slopes_df$se[j], 3), "\n",
                             "p-value: ", format.pval(slopes_df$p_value[j], digits = 3), "\n\n")
      }
      
      # Subscale results if they exist
      if(!is.null(results$subscales)) {
        output_text <- paste0(output_text, "\nSubscale Results:\n")
        
        for(sub_result in results$subscales) {
          output_text <- paste0(output_text,
                               "\nSubscale: ", sub_result$scale, "\n",
                               "Chi-square: ", round(sub_result$model_comparison$Chisq[2], 3), "\n",
                               "p-value: ", format.pval(sub_result$model_comparison$`Pr(>Chisq)`[2], digits = 3), "\n",
                               "FDR-adjusted p-value: ", format.pval(sub_result$adjusted_p, digits = 3), "\n",
                               "Effect size (RÂ² marginal): ", round(sub_result$effect_size, 3), "\n\n")
          
          # Simple slopes for subscales
          slopes_df <- sub_result$simple_slopes
          for(j in 1:nrow(slopes_df)) {
            output_text <- paste0(output_text,
                                "At ", slopes_df$moderator_level[j], " SD:\n",
                                "Slope: ", round(slopes_df$slope[j], 3), "\n",
                                "SE: ", round(slopes_df$se[j], 3), "\n",
                                "p-value: ", format.pval(slopes_df$p_value[j], digits = 3), "\n\n")
          }
        }
      }
    }
  }
  
  return(output_text)
}

# Generate and save results
results_text <- format_switch_consensus_results(model_results, 
                                              if(nrow(significant_results) > 0) moderation_results else NULL)

# Create output directory if it doesn't exist
dir.create(here("output", "behav"), showWarnings = FALSE, recursive = TRUE)

# Save to file
writeLines(results_text, here("output", "behav", "ch_accuracy_by_switch_and_consensus.txt"))
```

Plot Bet 1 magnitude as a function of switch difference across trials (Choice 2 - Choice 1) and group consensus (START HERE TOMORROW)

```{r}
# Read and process data
merged_data <- read_csv(here("data", "preprocessed", "merged_test_data.csv"), show_col_types = FALSE)

# Create separate datasets for switch and stay trials
switch_data <- merged_data %>% 
 filter(choice_switch_across_trials == 1) %>%
 mutate(
   consensus_numeric = case_when(
     player.choice1_with %in% c(0, 1) ~ 2,  # 4:0
     player.choice1_with %in% c(0.25, 0.75) ~ 1,  # 3:1
     player.choice1_with == 0.5 ~ 0  # 2:2
   ),
   direction = if_else(player.choice1_with > 0.5, "With group", "Against group")
 )

stay_data <- merged_data %>%
 filter(choice_switch_across_trials == 0) %>%
 mutate(
   consensus_numeric = case_when(
     player.choice1_with %in% c(0, 1) ~ 2,  # 4:0
     player.choice1_with %in% c(0.25, 0.75) ~ 1,  # 3:1
     player.choice1_with == 0.5 ~ 0  # 2:2
   ),
   direction = if_else(player.choice1_with > 0.5, "With group", "Against group")
 )

# Modified model running function
run_questionnaire_models <- function(quest_var, data_type) {
 print(paste("\nRunning models for:", quest_var, "in", data_type, "trials"))
 
 # Select appropriate dataset
 data <- if(data_type == "switch") switch_data else stay_data
 
 # Scale questionnaire variables
 data <- data %>%
   mutate(
     across(c(ssms, dass, lsas, srp_sf, ami, aq_10, age), scale),
     gender = factor(gender),
     direction = factor(direction)
   )
 
 # Run models
 model1 <- lmer(as.formula(paste("player.bet1 ~ direction * consensus_numeric *", 
                                quest_var, "+ age + (1|participant.id_in_session)")),
               data = data,
               control = lmerControl(optimizer = "bobyqa"))
 
 model2 <- lmer(as.formula(paste("player.bet1 ~ direction * consensus_numeric *", 
                                quest_var, "+ age + (1|participant.id_in_session) + (1|gender)")),
               data = data,
               control = lmerControl(optimizer = "bobyqa"))
 
 # Get best model
 models_list <- list(model1, model2)
 aic_values <- sapply(models_list, AIC)
 winning_model <- models_list[[which.min(aic_values)]]
 
 # Get model statistics
 model_anova <- car::Anova(winning_model, type = 2)
 interaction_p <- model_anova[paste0("direction:consensus_numeric:", quest_var), "Pr(>Chisq)"]
 
 list(
   questionnaire = quest_var,
   trial_type = data_type,
   winning_model = winning_model,
   p_value = interaction_p,
   aic = min(aic_values)
 )
}

# Custom theme
theme_custom <- theme_minimal() +
 theme(
   panel.grid.minor = element_blank(),
   legend.position = "right",
   plot.title = element_text(size = 12, face = "bold"),
   plot.subtitle = element_text(size = 10),
   axis.title = element_text(size = 10),
   legend.title = element_text(size = 10),
   legend.text = element_text(size = 9)
 )

# Continuous relationship plotting function
plot_continuous_relationship <- function(var, all_data, model_results) {
 # Combine data from both trial types
 subject_averages <- bind_rows(
   switch_data %>% mutate(trial_type = "Switch trials"),
   stay_data %>% mutate(trial_type = "Stay trials")
 ) %>%
   group_by(participant.id_in_session, direction, consensus_numeric, trial_type, .data[[var]]) %>%
   summarise(
     mean_bet = mean(player.bet1),
     .groups = 'drop'
   ) %>%
   mutate(
     consensus_level = case_when(
       consensus_numeric == 2 ~ "4:0",
       consensus_numeric == 1 ~ "3:1",
       consensus_numeric == 0 ~ "2:2"
     ),
     consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
   )
 
 # Calculate correlations for effect sizes
 effects <- subject_averages %>%
   group_by(direction, consensus_level, trial_type) %>%
   summarise(
     correlation = cor(.data[[var]], mean_bet),
     .groups = 'drop'
   )
 
 p <- ggplot(subject_averages, 
             aes(x = .data[[var]], 
                 y = mean_bet,
                 color = direction)) +
   geom_point(alpha = 0.5) +
   geom_smooth(method = "lm", formula = y ~ x) +
   facet_grid(trial_type ~ consensus_level) +
   labs(x = var,
        y = "Bet magnitude",
        title = paste("Relationship between", var, "and bet magnitude"),
        subtitle = paste("p =", format.pval(model_results$p_value[
          model_results$questionnaire == var], digits = 3)),
        caption = paste("Effect sizes (r) range:", 
                       round(min(effects$correlation), 3), "to",
                       round(max(effects$correlation), 3))) +
   scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
   theme_custom
 
 print(p)
}

# Run analyses for both conditions
questionnaire_vars <- c("ssms", "dass", "lsas", "srp_sf", "ami", "aq_10")
trial_types <- c("switch", "stay")

# Create empty list to store results
all_results <- list()

# Add simple slopes analysis function
analyze_simple_slopes <- function(model, questionnaire_var, trial_type) {
  # Get appropriate dataset
  data <- if(trial_type == "switch") switch_data else stay_data
  
  # Create probe points for the questionnaire variable
  probe_points <- data %>%
    summarise(
      low = mean(!!sym(questionnaire_var)) - sd(!!sym(questionnaire_var)),
      mean = mean(!!sym(questionnaire_var)),
      high = mean(!!sym(questionnaire_var)) + sd(!!sym(questionnaire_var))
    )
  
  # Create prediction grid
  pred_data <- expand.grid(
    direction = unique(data$direction),
    consensus_numeric = unique(data$consensus_numeric),
    quest_score = c(probe_points$low, probe_points$mean, probe_points$high)
  ) %>%
    mutate(
      quest_level = rep(c("-1 SD", "Mean", "+1 SD"), 
                       each = length(unique(data$direction)) * length(unique(data$consensus_numeric))),
      direction = factor(direction)
    )
  
  # Add other necessary variables at their means/reference levels
  pred_data$age <- 0  # since age is scaled, 0 is the mean
  
  # Create column with questionnaire variable name
  pred_data[[questionnaire_var]] <- pred_data$quest_score
  
  # Get predictions
  pred_data$predicted <- predict(model, newdata = pred_data, re.form = NA)
  
  # Convert consensus numeric to factor for plotting
  pred_data <- pred_data %>%
    mutate(
      consensus_level = case_when(
        consensus_numeric == 2 ~ "4:0",
        consensus_numeric == 1 ~ "3:1",
        consensus_numeric == 0 ~ "2:2"
      ),
      consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
    )
  
  # Create plot
  p <- ggplot(pred_data, 
              aes(x = consensus_level, 
                  y = predicted,
                  color = direction,
                  linetype = quest_level,
                  group = interaction(direction, quest_level))) +
    geom_line() +
    geom_point(size = 3) +
    labs(title = paste("Simple slopes analysis for", questionnaire_var, "in", trial_type, "trials"),
         x = "Consensus Level",
         y = "Predicted Bet Magnitude",
         color = "Direction",
         linetype = paste(questionnaire_var, "Level")) +
    scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
    theme_custom
  
  # Get model summary for slopes
  model_summary <- summary(model)
  
  return(list(
    predictions = pred_data,
    plot = p,
    model_summary = model_summary
  ))
}

# Run models for each combination
for(var in questionnaire_vars) {
  for(trial in trial_types) {
    result <- run_questionnaire_models(var, trial)
    all_results[[paste(var, trial, sep="_")]] <- tibble(
      questionnaire = result$questionnaire,
      trial_type = result$trial_type,
      p_value = result$p_value,
      p_adjusted = p.adjust(result$p_value, method = "fdr"),
      aic = result$aic
    )
  }
}

# Combine results into a single dataframe
model_results <- bind_rows(all_results)

# Print results
print("Model results for each questionnaire and trial type:")
print(model_results %>% 
        select(questionnaire, trial_type, p_value, p_adjusted) %>%
        mutate(across(where(is.numeric), ~round(., 3))))

# Create plots for significant relationships
significant_results <- model_results %>%
  filter(p_adjusted < 0.1)

if(nrow(significant_results) > 0) {
  for(i in 1:nrow(significant_results)) {
    var <- significant_results$questionnaire[i]
    trial_type <- significant_results$trial_type[i]
    
    # Continuous relationship plot
    plot_continuous_relationship(var, merged_data, model_results)
    
    # Median-split plot
    summary_data <- bind_rows(
      switch_data %>% mutate(trial_type = "Switch trials"),
      stay_data %>% mutate(trial_type = "Stay trials")
    ) %>%
      mutate(
        consensus_level = case_when(
          consensus_numeric == 2 ~ "4:0",
          consensus_numeric == 1 ~ "3:1",
          consensus_numeric == 0 ~ "2:2"
        ),
        consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0")),
        quest_group = ifelse(.data[[var]] > median(.data[[var]]), "High", "Low")
      ) %>%
      group_by(consensus_level, direction, quest_group, trial_type) %>%
      summarise(
        mean_bet = mean(player.bet1),
        se = sd(player.bet1) / sqrt(n()),
        .groups = 'drop'
      )
    
    p_median <- ggplot(summary_data, 
                      aes(x = consensus_level, 
                          y = mean_bet, 
                          color = direction,
                          linetype = quest_group,
                          group = interaction(direction, quest_group))) +
      geom_line() +
      geom_point() +
      geom_errorbar(aes(ymin = mean_bet - se, 
                        ymax = mean_bet + se), 
                    width = 0.2) +
      facet_grid(trial_type ~ .) +
      scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
      labs(x = "Group consensus",
           y = "Bet magnitude",
           title = paste("Effect of", var, "(Median-split)"),
           subtitle = paste("p =", format.pval(significant_results$p_value[i], digits = 3))) +
      theme_custom
    
    print(p_median)
    
    # Run and plot simple slopes analysis
    result <- run_questionnaire_models(var, trial_type)
    slopes_analysis <- analyze_simple_slopes(result$winning_model, var, trial_type)
    
    cat("\nSimple slopes analysis for", var, "in", trial_type, ":\n")
    print(slopes_analysis$model_summary)
    print(slopes_analysis$plot)
  }
} else {
  print("No significant relationships found after multiple comparison correction")
}

################## MODERATION FOR SIGNIFICANT RESULTS ONLY ###################

# Define subscale mapping
subscale_mapping <- list(
  lsas = c("lsas_p", "lsas_s"),
  dass = c("dass_a", "dass_d", "dass_s"),
  ssms = c("ssms_cd", "ssms_ia"),
  srp_sf = c("srp_sf_ipm", "srp_sf_ca", "srp_sf_els", "srp_sf_ct"),
  ami = c("ami_es", "ami_sm", "ami_ba"),
  aq_10 = NULL
)

# Extract and scale the subscale data at participant level
subscale_data <- merged_data %>%
  group_by(participant.id_in_session) %>%
  slice(1) %>%
  select(participant.id_in_session,
         lsas_p, lsas_s,
         dass_a, dass_d, dass_s,
         ssms_cd, ssms_ia,
         srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
         ami_es, ami_sm, ami_ba) 

# Scale the columns separately
subscale_data_scaled <- subscale_data %>%
  ungroup() %>%
  mutate(across(-participant.id_in_session, ~as.vector(scale(.x))))

# Define function for moderation analysis
run_moderation_analysis <- function(data, scale_name, subscales = NULL, trial_type) {
  # Run moderation for main scale
  main_model <- lmer(player.bet1 ~ direction * consensus_numeric * scale_name + 
                    age + (1|participant.id_in_session),
                    data = data,
                    control = lmerControl(optimizer = "bobyqa"))
  
  results <- list(main = list(
    scale = scale_name,
    model = main_model,
    summary = summary(main_model)
  ))
  
  # Run for subscales if they exist
  if (!is.null(subscales)) {
    subscale_results <- map(subscales, function(subscale) {
      if(subscale %in% names(data)) {  # Only run if subscale exists in data
        sub_model <- lmer(as.formula(paste(
          "player.bet1 ~ direction * consensus_numeric *", subscale,
          "+ age + (1|participant.id_in_session)"
        )),
        data = data,
        control = lmerControl(optimizer = "bobyqa"))
        
        list(
          scale = subscale,
          model = sub_model,
          summary = summary(sub_model)
        )
      } else {
        NULL
      }
    })
    results$subscales <- compact(subscale_results)  # Remove NULL results
  }
  
  return(results)
}

# Join scaled subscales to each dataset
switch_data_with_subscales <- switch_data %>%
  left_join(subscale_data_scaled, by = "participant.id_in_session")

stay_data_with_subscales <- stay_data %>%
  left_join(subscale_data_scaled, by = "participant.id_in_session")

# Run the moderation analysis for significant results
if(nrow(significant_results) > 0) {
  moderation_results <- map2(significant_results$questionnaire, 
                           significant_results$trial_type,
                           function(scale, trial_type) {
    subscales <- subscale_mapping[[scale]]
    
    # Select appropriate dataset
    data <- if(trial_type == "switch") {
      switch_data_with_subscales
    } else {
      stay_data_with_subscales
    }
    
    # Prepare data with scale and subscales
    mod_data <- data %>%
      rename(scale_name = !!scale)
    
    run_moderation_analysis(mod_data, "scale_name", subscales, trial_type)
  })
  
  # Print results and create visualization
  for(i in seq_along(moderation_results)) {
    scale_name <- significant_results$questionnaire[i]
    trial_type <- significant_results$trial_type[i]
    results <- moderation_results[[i]]
    
    cat("\nModeration Analysis Results for", scale_name, "in", trial_type, "trials\n")
    cat("Main Scale Results:\n")
    print(car::Anova(results$main$model, type = 2))
    
    if(!is.null(results$subscales)) {
      cat("\nSubscale Results:\n")
      for(sub_result in results$subscales) {
        cat("\nSubscale:", sub_result$scale, "\n")
        print(car::Anova(sub_result$model, type = 2))
      }
    }
    
    # Create visualization for significant moderations
    plot_moderation <- function(model, title) {
      # Create data frame for predictions
      new_data <- expand.grid(
        direction = unique(model@frame$direction),
        consensus_numeric = unique(model@frame$consensus_numeric),
        scale_name = c(-1, 0, 1),
        age = 0  # mean of scaled age
      )
      
      # Get predictions
      new_data$predicted <- predict(model, newdata = new_data, re.form = NA)
      
      # Convert consensus numeric to factor for plotting
      new_data <- new_data %>%
        mutate(
          consensus_level = case_when(
            consensus_numeric == 2 ~ "4:0",
            consensus_numeric == 1 ~ "3:1",
            consensus_numeric == 0 ~ "2:2"
          ),
          consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
        )
      
      # Create plot
      ggplot(new_data, 
             aes(x = consensus_level, 
                 y = predicted, 
                 color = direction,
                 linetype = as.factor(scale_name),
                 group = interaction(direction, scale_name))) +
        geom_line() +
        geom_point() +
        scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
        scale_linetype_discrete(name = "Scale Score",
                              labels = c("-1 SD", "Mean", "+1 SD")) +
        theme_custom +
        labs(title = title,
             x = "Consensus Group",
             y = "Predicted Bet Magnitude")
    }
    
    print(plot_moderation(results$main$model, 
                         paste("Moderation Effect of", scale_name, "in", trial_type, "trials")))
  }
} else {
  print("No significant results to analyze with moderation")
}
```

## Supplementary analyses

Bet difference as a function of choice switching on the current trial and group consensus (Fig Supp 2A)

```{r}
# Read and process data
df <- read_csv(here("data", "preprocessed", "merged_test_data.csv"), show_col_types = FALSE)

# Calculate bet difference and set up variables 
df <- df %>%
mutate(
  bet_difference = player.bet2 - player.bet1,
  consensus_numeric = case_when(
    player.choice1_with %in% c(0, 1) ~ 2,  # 4:0
    player.choice1_with %in% c(0.25, 0.75) ~ 1,  # 3:1
    player.choice1_with == 0.5 ~ 0  # 2:2
  ),
  direction = if_else(player.choice1_with > 0.5, "With group", "Against group")
)

# Modified model running function
run_questionnaire_models <- function(quest_var, data) {
 print(paste("\nRunning models for:", quest_var))
 
 # Scale questionnaire variables and prepare data
 analysis_data <- data %>%
   mutate(
     across(c(ssms, dass, lsas, srp_sf, ami, aq_10, age), scale),
     gender = factor(gender),
     direction = factor(direction),
     switch_vs_stay = factor(player.switch_vs_stay),
     participant.id_in_session = factor(participant.id_in_session)
   )
 
 # Run models
 model1 <- lmer(as.formula(paste("bet_difference ~ direction * consensus_numeric * switch_vs_stay *", 
                                quest_var, "+ age + (1|participant.id_in_session)")),
               data = analysis_data,
               control = lmerControl(optimizer = "bobyqa"))
 
 model2 <- lmer(as.formula(paste("bet_difference ~ direction * consensus_numeric * switch_vs_stay *", 
                                quest_var, "+ age + (1|participant.id_in_session) + (1|gender)")),
               data = analysis_data,
               control = lmerControl(optimizer = "bobyqa"))
 
 # Get best model
 models_list <- list(model1, model2)
 aic_values <- sapply(models_list, AIC)
 winning_model <- models_list[[which.min(aic_values)]]
 
 # Get model statistics
 model_anova <- car::Anova(winning_model, type = 2)
 interaction_p <- model_anova[paste0("direction:consensus_numeric:switch_vs_stay:", quest_var), "Pr(>Chisq)"]
 
 list(
   questionnaire = quest_var,
   winning_model = winning_model,
   p_value = interaction_p,
   aic = min(aic_values)
 )
}

# Custom theme
theme_custom <- theme_minimal() +
 theme(
   panel.grid.minor = element_blank(),
   legend.position = "right",
   plot.title = element_text(size = 12, face = "bold"),
   plot.subtitle = element_text(size = 10),
   axis.title = element_text(size = 10),
   legend.title = element_text(size = 10),
   legend.text = element_text(size = 9)
 )

# Continuous relationship plotting function
plot_continuous_relationship <- function(var, data, model_results) {
 # Calculate subject-level averages
 subject_averages <- data %>%
   group_by(participant.id_in_session, direction, consensus_numeric, 
            player.switch_vs_stay, .data[[var]]) %>%
   summarise(
     mean_diff = mean(bet_difference),
     .groups = 'drop'
   ) %>%
   mutate(
     consensus_level = case_when(
       consensus_numeric == 2 ~ "4:0",
       consensus_numeric == 1 ~ "3:1",
       consensus_numeric == 0 ~ "2:2"
     ),
     consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
   )
 
 # Calculate correlations for effect sizes
 effects <- subject_averages %>%
   group_by(direction, consensus_level, player.switch_vs_stay) %>%
   summarise(
     correlation = cor(.data[[var]], mean_diff),
     .groups = 'drop'
   )
 
 p <- ggplot(subject_averages, 
             aes(x = .data[[var]], 
                 y = mean_diff,
                 color = direction)) +
   geom_point(alpha = 0.5) +
   geom_smooth(method = "lm", formula = y ~ x) +
   facet_grid(player.switch_vs_stay ~ consensus_level, 
              labeller = labeller(player.switch_vs_stay = 
                c("0" = "Stay trials", "1" = "Switch trials"))) +
   labs(x = var,
        y = "Bet difference (Bet 2 - Bet 1)",
        title = paste("Relationship between", var, "and bet difference"),
        subtitle = paste("p =", format.pval(model_results$p_value[
          model_results$questionnaire == var], digits = 3)),
        caption = paste("Effect sizes (r) range:", 
                       round(min(effects$correlation, na.rm = TRUE), 3), "to",
                       round(max(effects$correlation, na.rm = TRUE), 3))) +
   scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
   theme_custom
 
 print(p)
}

# Median-split plotting function 
plot_median_split <- function(var, data, model_results) {
 summary_data <- data %>%
   mutate(
     consensus_level = case_when(
       consensus_numeric == 2 ~ "4:0",
       consensus_numeric == 1 ~ "3:1",
       consensus_numeric == 0 ~ "2:2"
     ),
     consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0")),
     quest_group = ifelse(.data[[var]] > median(.data[[var]]), "High", "Low")
   ) %>%
   group_by(consensus_level, direction, quest_group, player.switch_vs_stay) %>%
   summarise(
     mean_diff = mean(bet_difference),
     se = sd(bet_difference) / sqrt(n()),
     .groups = 'drop'
   )
 
 p <- ggplot(summary_data, 
             aes(x = consensus_level, 
                 y = mean_diff, 
                 color = direction,
                 linetype = quest_group,
                 group = interaction(direction, quest_group))) +
   geom_line() +
   geom_point() +
   geom_errorbar(aes(ymin = mean_diff - se, 
                     ymax = mean_diff + se), 
                 width = 0.2) +
   facet_wrap(~player.switch_vs_stay,
              labeller = labeller(player.switch_vs_stay = 
                c("0" = "Stay trials", "1" = "Switch trials"))) +
   scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
   labs(x = "Group consensus",
        y = "Bet difference (Bet 2 - Bet 1)",
        title = paste("Effect of", var, "(Median-split)"),
        subtitle = paste("p =", format.pval(model_results$p_value[
          model_results$questionnaire == var], digits = 3))) +
   theme_custom
 
 print(p)
}

# Add simple slopes analysis function
analyze_simple_slopes <- function(model, questionnaire_var) {
  # Create probe points for the questionnaire variable
  probe_points <- df %>%
    summarise(
      low = mean(!!sym(questionnaire_var)) - sd(!!sym(questionnaire_var)),
      mean = mean(!!sym(questionnaire_var)),
      high = mean(!!sym(questionnaire_var)) + sd(!!sym(questionnaire_var))
    )
  
  # Create prediction grid
  pred_data <- expand.grid(
    direction = unique(df$direction),
    consensus_numeric = unique(df$consensus_numeric),
    switch_vs_stay = c(0, 1),
    quest_score = c(probe_points$low, probe_points$mean, probe_points$high)
  ) %>%
    mutate(
      quest_level = rep(c("-1 SD", "Mean", "+1 SD"), 
                       each = length(unique(df$direction)) * 
                         length(unique(df$consensus_numeric)) * 2),
      direction = factor(direction),
      switch_vs_stay = factor(switch_vs_stay)
    )
  
  # Add other necessary variables at their means/reference levels
  pred_data$age <- 0  # since age is scaled, 0 is the mean
  
  # Create column with questionnaire variable name
  pred_data[[questionnaire_var]] <- pred_data$quest_score
  
  # Get predictions
  pred_data$predicted <- predict(model, newdata = pred_data, re.form = NA)
  
  # Convert consensus numeric to factor for plotting
  pred_data <- pred_data %>%
    mutate(
      consensus_level = case_when(
        consensus_numeric == 2 ~ "4:0",
        consensus_numeric == 1 ~ "3:1",
        consensus_numeric == 0 ~ "2:2"
      ),
      consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
    )
  
  # Create separate plots for switch and stay
  plots <- list()
  
  for(choice in c("0", "1")) {
    plot_data <- pred_data %>% filter(switch_vs_stay == choice)
    
    p <- ggplot(plot_data, 
                aes(x = consensus_level, 
                    y = predicted,
                    color = direction,
                    linetype = quest_level,
                    group = interaction(direction, quest_level))) +
      geom_line() +
      geom_point(size = 3) +
      labs(title = paste("Simple slopes analysis for", questionnaire_var, 
                        "\nin", if(choice == "0") "Stay" else "Switch", "trials"),
           x = "Consensus Level",
           y = "Predicted Bet Difference",
           color = "Direction",
           linetype = paste(questionnaire_var, "Level")) +
      scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
      theme_custom
    
    plots[[choice]] <- p
  }
  
  # Get model summary for slopes
  model_summary <- summary(model)
  
  return(list(
    predictions = pred_data,
    plots = plots,
    model_summary = model_summary
  ))
}

# Add simple slopes analysis function
analyze_simple_slopes <- function(model, questionnaire_var) {
  # Create probe points for the questionnaire variable
  probe_points <- df %>%
    summarise(
      low = mean(!!sym(questionnaire_var)) - sd(!!sym(questionnaire_var)),
      mean = mean(!!sym(questionnaire_var)),
      high = mean(!!sym(questionnaire_var)) + sd(!!sym(questionnaire_var))
    )
  
  # Create prediction grid
  pred_data <- expand.grid(
    direction = unique(df$direction),
    consensus_numeric = unique(df$consensus_numeric),
    switch_vs_stay = c(0, 1),
    quest_score = c(probe_points$low, probe_points$mean, probe_points$high)
  ) %>%
    mutate(
      quest_level = rep(c("-1 SD", "Mean", "+1 SD"), 
                       each = length(unique(df$direction)) * 
                         length(unique(df$consensus_numeric)) * 2),
      direction = factor(direction),
      switch_vs_stay = factor(switch_vs_stay)
    )
  
  # Add other necessary variables at their means/reference levels
  pred_data$age <- 0  # since age is scaled, 0 is the mean
  
  # Create column with questionnaire variable name
  pred_data[[questionnaire_var]] <- pred_data$quest_score
  
  # Get predictions
  pred_data$predicted <- predict(model, newdata = pred_data, re.form = NA)
  
  # Convert consensus numeric to factor for plotting
  pred_data <- pred_data %>%
    mutate(
      consensus_level = case_when(
        consensus_numeric == 2 ~ "4:0",
        consensus_numeric == 1 ~ "3:1",
        consensus_numeric == 0 ~ "2:2"
      ),
      consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
    )
  
  # Create separate plots for switch and stay
  plots <- list()
  
  for(choice in c("0", "1")) {
    plot_data <- pred_data %>% filter(switch_vs_stay == choice)
    
    p <- ggplot(plot_data, 
                aes(x = consensus_level, 
                    y = predicted,
                    color = direction,
                    linetype = quest_level,
                    group = interaction(direction, quest_level))) +
      geom_line() +
      geom_point(size = 3) +
      labs(title = paste("Simple slopes analysis for", questionnaire_var, 
                        "\nin", if(choice == "0") "Stay" else "Switch", "trials"),
           x = "Consensus Level",
           y = "Predicted Bet Difference",
           color = "Direction",
           linetype = paste(questionnaire_var, "Level")) +
      scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
      theme_custom
    
    plots[[choice]] <- p
  }
  
  # Get model summary for slopes
  model_summary <- summary(model)
  
  return(list(
    predictions = pred_data,
    plots = plots,
    model_summary = model_summary
  ))
}

# Run analyses for questionnaires
questionnaire_vars <- c("ssms", "dass", "lsas", "srp_sf", "ami", "aq_10")

# Create empty list to store results
all_results <- list()

# Run models for each questionnaire
for(var in questionnaire_vars) {
  result <- run_questionnaire_models(var, df)
  all_results[[var]] <- tibble(
    questionnaire = result$questionnaire,
    p_value = result$p_value,
    p_adjusted = p.adjust(result$p_value, method = "fdr"),
    aic = result$aic
  )
}

# Combine results into a single dataframe
model_results <- bind_rows(all_results)

# Print results
print("Model results for each questionnaire:")
print(model_results %>% 
        select(questionnaire, p_value, p_adjusted) %>%
        mutate(across(where(is.numeric), ~round(., 3))))

# Create plots for significant relationships
significant_results <- model_results %>%
  filter(p_adjusted < 0.2)  # Using 0.5 for testing as requested

if(nrow(significant_results) > 0) {
  for(i in 1:nrow(significant_results)) {
    var <- significant_results$questionnaire[i]
    
    # Continuous relationship plot
    plot_continuous_relationship(var, df, model_results)
    
    # Median-split plot
    plot_median_split(var, df, model_results)
    
    # Run and plot simple slopes analysis
    result <- run_questionnaire_models(var, df)
    slopes_analysis <- analyze_simple_slopes(result$winning_model, var)
    
    cat("\nSimple slopes analysis for", var, ":\n")
    print(slopes_analysis$model_summary)
    # Print both switch and stay plots
    print(slopes_analysis$plots[["0"]])  # Stay trials
    print(slopes_analysis$plots[["1"]])  # Switch trials
  }
} else {
  print("No significant relationships found after multiple comparison correction")
}

################## MODERATION FOR SIGNIFICANT RESULTS ONLY ###################

# Define subscale mapping
subscale_mapping <- list(
  lsas = c("lsas_p", "lsas_s"),
  dass = c("dass_a", "dass_d", "dass_s"),
  ssms = c("ssms_cd", "ssms_ia"),
  srp_sf = c("srp_sf_ipm", "srp_sf_ca", "srp_sf_els", "srp_sf_ct"),
  ami = c("ami_es", "ami_sm", "ami_ba"),
  aq_10 = NULL
)

# Extract and scale the subscale data at participant level
subscale_data <- df %>%
  group_by(participant.id_in_session) %>%
  slice(1) %>%
  select(participant.id_in_session,
         lsas_p, lsas_s,
         dass_a, dass_d, dass_s,
         ssms_cd, ssms_ia,
         srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
         ami_es, ami_sm, ami_ba) 

# Scale the columns separately
subscale_data_scaled <- subscale_data %>%
  ungroup() %>%
  mutate(across(-participant.id_in_session, ~as.vector(scale(.x))))

# First, ensure all categorical variables are properly factored in df_with_subscales
df_with_subscales <- df %>%
  left_join(subscale_data_scaled, by = "participant.id_in_session") %>%
  mutate(
    switch_vs_stay = factor(player.switch_vs_stay, levels = c(0, 1)),  # Create from player.switch_vs_stay
    direction = factor(direction, levels = c("Against group", "With group")),
    consensus_numeric = factor(consensus_numeric, levels = c(0, 1, 2)),
    participant.id_in_session = factor(participant.id_in_session)
  )

# Modify the moderation analysis function
run_moderation_analysis <- function(data, scale_name, subscales = NULL) {
  # Ensure factors are properly coded
  data <- data %>%
    mutate(
      direction = as.factor(direction),
      consensus_numeric = as.factor(consensus_numeric),
      switch_vs_stay = as.factor(switch_vs_stay),
      participant.id_in_session = as.factor(participant.id_in_session)
    )
  
  # Main model
  main_model <- lmer(bet_difference ~ direction * consensus_numeric * switch_vs_stay * scale_name + 
                    age + (1|participant.id_in_session),
                    data = data,
                    control = lmerControl(optimizer = "bobyqa"))
  
  results <- list(main = list(
    scale = scale_name,
    model = main_model,
    summary = summary(main_model)
  ))
  
  # Subscales analysis
  if (!is.null(subscales)) {
    subscale_results <- map(subscales, function(subscale) {
      if(subscale %in% names(data)) {
        formula_str <- paste("bet_difference ~ direction * consensus_numeric * switch_vs_stay *", 
                           subscale, "+ age + (1|participant.id_in_session)")
        
        sub_model <- lmer(as.formula(formula_str),
                         data = data,
                         control = lmerControl(optimizer = "bobyqa"))
        
        list(
          scale = subscale,
          model = sub_model,
          summary = summary(sub_model)
        )
      } else {
        NULL
      }
    })
    results$subscales <- compact(subscale_results)
  }
  
  return(results)
}

# Let's also check the data before running the analysis
print("Data structure before analysis:")
print(str(df_with_subscales[c("bet_difference", "direction", "consensus_numeric", 
                             "switch_vs_stay", "age", "participant.id_in_session")]))

# Try running the analysis with the additional debugging information
if(nrow(significant_results) > 0) {
  moderation_results <- map(significant_results$questionnaire,
                           function(scale) {
    print(paste("\nAnalyzing scale:", scale))
    subscales <- subscale_mapping[[scale]]
    
    # Prepare data with scale and subscales
    mod_data <- df_with_subscales %>%
      rename(scale_name = !!scale)
    
    run_moderation_analysis(mod_data, "scale_name", subscales)
  })
}

# Then modify the mapping section:
if(nrow(significant_results) > 0) {
  moderation_results <- map(significant_results$questionnaire,
                           function(scale) {
    subscales <- subscale_mapping[[scale]]
    
    # Prepare data with scale and subscales
    mod_data <- df_with_subscales %>%
      rename(scale_name = !!scale)
    
    run_moderation_analysis(mod_data, "scale_name", subscales)
  })
  
  # Print results and create visualization
  for(i in seq_along(moderation_results)) {
    scale_name <- significant_results$questionnaire[i]
    results <- moderation_results[[i]]
    
    cat("\nModeration Analysis Results for", scale_name, "\n")
    cat("Main Scale Results:\n")
    print(car::Anova(results$main$model, type = 2))
    
    if(!is.null(results$subscales)) {
      cat("\nSubscale Results:\n")
      for(sub_result in results$subscales) {
        cat("\nSubscale:", sub_result$scale, "\n")
        print(car::Anova(sub_result$model, type = 2))
      }
    }
    
    # Create visualization for significant moderations
    plot_moderation <- function(model, title) {
      # Create data frame for predictions
      new_data <- expand.grid(
        direction = unique(model@frame$direction),
        consensus_numeric = unique(model@frame$consensus_numeric),
        switch_vs_stay = factor(c(0, 1)),  # Explicitly make this a factor
        scale_name = c(-1, 0, 1),  # Keep this numeric since it's a continuous predictor
        age = 0  # mean of scaled age
      ) %>%
        mutate(
          direction = factor(direction, levels = levels(model@frame$direction)),
          consensus_numeric = factor(consensus_numeric, levels = levels(model@frame$consensus_numeric)),
          switch_vs_stay = factor(switch_vs_stay, levels = c("0", "1"))
        )
      
      # Get predictions
      new_data$predicted <- predict(model, newdata = new_data, re.form = NA)
      
      # Convert consensus numeric to factor for plotting
      new_data <- new_data %>%
        mutate(
          consensus_level = case_when(
            consensus_numeric == 2 ~ "4:0",
            consensus_numeric == 1 ~ "3:1",
            consensus_numeric == 0 ~ "2:2"
          ),
          consensus_level = factor(consensus_level, levels = c("2:2", "3:1", "4:0"))
        )
      
      # Create separate plots for switch and stay
      plots <- list()
      
      for(choice in c("0", "1")) {
        plot_data <- new_data %>% filter(switch_vs_stay == choice)
        
        p <- ggplot(plot_data, 
                   aes(x = consensus_level, 
                       y = predicted, 
                       color = direction,
                       linetype = factor(scale_name),
                       group = interaction(direction, scale_name))) +
          geom_line() +
          geom_point() +
          scale_color_manual(values = c("Against group" = "red", "With group" = "blue")) +
          scale_linetype_discrete(name = "Scale Score",
                                labels = c("-1 SD", "Mean", "+1 SD")) +
          theme_custom +
          labs(title = paste(title, "\nin", if(choice == "0") "Stay" else "Switch", "trials"),
               x = "Consensus Group",
               y = "Predicted Bet Difference")
        
        plots[[choice]] <- p
      }
      
      return(plots)
    }
    
    plots <- plot_moderation(results$main$model, 
                           paste("Moderation Effect of", scale_name))
    
    # Print both plots
    print(plots[["0"]])  # Stay trials
    print(plots[["1"]])  # Switch trials
  }
} else {
  print("No significant results to analyze with moderation")
}
```


### Choice accuracy and bet magnitude relative to reversal (Figs S2-B-C)

Choice accuracy relative to reversal (plus and minus 3 trials)

```{r}
# Read data
df <- read_csv(here("data", "preprocessed", "merged_test_data.csv"), show_col_types = FALSE)

# First, let's see when reversals occur
reversal_trials <- df %>% 
  filter(group.reversal_happened == 1) %>%
  select(group.trial_number) %>%
  distinct()
print("Reversal occurs on trials:")
print(reversal_trials)

# Prepare base questionnaire data first
base_df <- df %>%
  distinct(participant.id_in_session, .keep_all = TRUE) %>%
  select(participant.id_in_session, 
         # Main scales
         ssms, dass, lsas, srp_sf, ami, aq_10,
         # Subscales
         lsas_p, lsas_s,
         dass_a, dass_d, dass_s,
         ssms_cd, ssms_ia,
         srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
         ami_es, ami_sm, ami_ba,
         # Demographics
         age, gender)

# Now create trial-level data and join with questionnaire data
analysis_df <- df %>%
  select(
    participant.id_in_session,
    group.trial_number,
    player.choice1_accuracy,
    player.choice2_accuracy,
    group.reversal_happened
  ) %>%
  # Get trials relative to reversal and check for complete windows
  group_by(participant.id_in_session) %>%
  mutate(
    # For each trial, get its position relative to the closest reversal
    trial_to_reversal = map_dbl(group.trial_number, function(x) {
      reversal_trial <- reversal_trials$group.trial_number
      relative_pos <- x - reversal_trial
      if(any(abs(relative_pos) <= 3)) {
        relative_pos[which.min(abs(relative_pos))]
      } else {
        NA
      }
    }),
    # For each trial within a window, check if all 7 trials exist
    complete_window = map_lgl(group.trial_number, function(x) {
      reversal_trial <- reversal_trials$group.trial_number
      rel_pos <- x - reversal_trial
      if(any(abs(rel_pos) <= 3)) {
        closest_reversal <- reversal_trial[which.min(abs(rel_pos))]
        required_trials <- (closest_reversal - 3):(closest_reversal + 3)
        all(required_trials %in% group.trial_number)
      } else {
        FALSE
      }
    })
  ) %>%
  # Keep only complete windows
  filter(!is.na(trial_to_reversal) & complete_window) %>%
  # Reshape to long format
  pivot_longer(
    cols = c(player.choice1_accuracy, player.choice2_accuracy),
    names_to = "choice_type",
    values_to = "accuracy"
  ) %>%
  ungroup() %>%
  # Join with questionnaire data
  left_join(base_df %>% select(participant.id_in_session, ssms, dass, lsas, srp_sf, ami, aq_10, age, gender), 
            by = "participant.id_in_session") %>%
  mutate(
    choice_type = factor(choice_type, 
                        levels = c("player.choice1_accuracy", "player.choice2_accuracy"),
                        labels = c("Choice 1", "Choice 2")),
    trial_to_reversal = factor(trial_to_reversal),
    participant.id_in_session = factor(participant.id_in_session),
    # Scale questionnaire variables
    across(c(ssms, dass, lsas, srp_sf, ami, aq_10, age), scale)
  )

# Modified model running function
run_questionnaire_models <- function(quest_var, data) {
  print(paste("\nRunning models for:", quest_var))
  
  # Run models
  model1 <- lmer(as.formula(paste("accuracy ~ choice_type * trial_to_reversal *", 
                                 quest_var, "+ age + (1|participant.id_in_session)")),
                data = data,
                control = lmerControl(optimizer = "bobyqa"))
  
  model2 <- lmer(as.formula(paste("accuracy ~ choice_type * trial_to_reversal *", 
                                 quest_var, "+ age + (1|participant.id_in_session) + (1|gender)")),
                data = data,
                control = lmerControl(optimizer = "bobyqa"))
  
  # Get best model
  models_list <- list(model1, model2)
  aic_values <- sapply(models_list, AIC)
  winning_model <- models_list[[which.min(aic_values)]]
  
  # Get model statistics
  model_anova <- car::Anova(winning_model, type = 2)
  interaction_p <- model_anova[paste0("choice_type:trial_to_reversal:", quest_var), "Pr(>Chisq)"]
  
  list(
    questionnaire = quest_var,
    winning_model = winning_model,
    p_value = interaction_p,
    aic = min(aic_values)
  )
}

# Custom theme
theme_custom <- theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "right",
    plot.title = element_text(size = 12, face = "bold"),
    plot.subtitle = element_text(size = 10),
    axis.title = element_text(size = 10),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9)
  )

# Run analyses
questionnaire_vars <- c("ssms", "dass", "lsas", "srp_sf", "ami", "aq_10")

# Create empty list to store results
all_results <- list()

# Run models for each questionnaire
for(var in questionnaire_vars) {
  result <- run_questionnaire_models(var, analysis_df)
  all_results[[var]] <- tibble(
    questionnaire = result$questionnaire,
    p_value = result$p_value,
    p_adjusted = p.adjust(result$p_value, method = "fdr"),
    aic = result$aic
  )
}

# Combine results into a single dataframe
model_results <- bind_rows(all_results)

# Print results
print("Model results for each questionnaire:")
print(model_results %>% 
        select(questionnaire, p_value, p_adjusted) %>%
        mutate(across(where(is.numeric), ~round(., 3))))

# Create plots for significant relationships
significant_results <- model_results %>%
  filter(p_adjusted < 0.5)

if(nrow(significant_results) > 0) {
  for(i in 1:nrow(significant_results)) {
    var <- significant_results$questionnaire[i]
    
    # Median-split plot
    summary_data <- analysis_df %>%
      mutate(
        quest_group = ifelse(.data[[var]] > median(.data[[var]]), "High", "Low")
      ) %>%
      group_by(trial_to_reversal, choice_type, quest_group) %>%
      summarise(
        mean_accuracy = mean(accuracy),
        se = sd(accuracy) / sqrt(n()),
        .groups = 'drop'
      )
    
    # Create separate plots for High and Low groups
    p <- ggplot(summary_data, 
                aes(x = trial_to_reversal, 
                    y = mean_accuracy, 
                    color = choice_type,
                    group = choice_type)) +
      geom_line() +
      geom_point() +
      geom_errorbar(aes(ymin = mean_accuracy - se, 
                        ymax = mean_accuracy + se), 
                    width = 0.2) +
      facet_wrap(~quest_group) +
      scale_color_manual(values = c("Choice 1" = "lightblue", "Choice 2" = "darkblue")) +
      labs(x = "Trial relative to reversal",
           y = "Accuracy",
           title = paste("Effect of", var, "(Median-split)"),
           subtitle = paste("p =", format.pval(significant_results$p_value[i], digits = 3))) +
      theme_custom
    
    print(p)
  }
} else {
  print("No significant relationships found after multiple comparison correction")
}

################## MODERATION FOR SIGNIFICANT RESULTS ONLY ###################

# Define subscale mapping
subscale_mapping <- list(
  lsas = c("lsas_p", "lsas_s"),
  dass = c("dass_a", "dass_d", "dass_s"),
  ssms = c("ssms_cd", "ssms_ia"),
  srp_sf = c("srp_sf_ipm", "srp_sf_ca", "srp_sf_els", "srp_sf_ct"),
  ami = c("ami_es", "ami_sm", "ami_ba"),
  aq_10 = NULL
)

# When extracting subscale data, convert participant ID to factor to match analysis_df
subscale_data <- df %>%
  group_by(participant.id_in_session) %>%
  slice(1) %>%
  select(participant.id_in_session,
         lsas_p, lsas_s,
         dass_a, dass_d, dass_s,
         ssms_cd, ssms_ia,
         srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
         ami_es, ami_sm, ami_ba) 

# Scale the subscales and convert participant ID to factor
subscale_data_scaled <- subscale_data %>%
  ungroup() %>%
  mutate(
    across(-participant.id_in_session, ~as.vector(scale(.x))),
    participant.id_in_session = factor(participant.id_in_session)  # Convert to factor
  )

# Define function for running moderation analysis
run_subscale_moderation <- function(scale_name, analysis_df, subscales) {
  results <- list()
  
  for(subscale in subscales) {
    print(paste("\nRunning moderation analysis for subscale:", subscale))
    
    # Use actual column name as it appears in data
    formula_str <- paste0("accuracy ~ choice_type * trial_to_reversal * ", subscale, 
                         " + age + (1|participant.id_in_session)")
    
    print(paste("Trying to run model with formula:", formula_str))
    
    model <- lmer(as.formula(formula_str),
                 data = analysis_df,
                 control = lmerControl(optimizer = "bobyqa"))
    
    # Get model statistics
    model_anova <- car::Anova(model, type = 2)
    interaction_term <- paste0("choice_type:trial_to_reversal:", subscale)
    interaction_p <- model_anova[interaction_term, "Pr(>Chisq)"]
    
    results[[subscale]] <- list(
      subscale = subscale,
      model = model,
      p_value = interaction_p
    )
  }
  
  return(results)
}

# Join scaled subscales to analysis_df
analysis_df_with_subscales <- analysis_df %>%
  left_join(subscale_data_scaled, by = "participant.id_in_session")

# After the join, add this check:
print("Checking columns in analysis_df_with_subscales:")
print(names(analysis_df_with_subscales))

# Run moderation for significant results
significant_scales <- model_results %>%
  filter(p_adjusted < 0.5) %>%
  pull(questionnaire)

if(length(significant_scales) > 0) {
  moderation_results <- list()
  
  for(scale in significant_scales) {
    subscales <- subscale_mapping[[scale]]
    if(!is.null(subscales)) {
      moderation_results[[scale]] <- run_subscale_moderation(
        scale, 
        analysis_df_with_subscales, 
        subscales
      )
      
      # Print results
      cat("\nModeration Results for", scale, "subscales:\n")
      subscale_summary <- map_df(moderation_results[[scale]], function(result) {
        tibble(
          subscale = result$subscale,
          p_value = result$p_value,
          p_adjusted = p.adjust(result$p_value, method = "fdr")
        )
      })
      print(subscale_summary)
      
      # Create plots for significant subscale moderations
      significant_subscales <- subscale_summary %>%
        filter(p_adjusted < 0.5) %>%
        pull(subscale)
      
      for(subscale in significant_subscales) {
        summary_data <- analysis_df_with_subscales %>%
          mutate(
            subscale_group = ifelse(.data[[subscale]] > median(.data[[subscale]]), 
                                  "High", "Low")
          ) %>%
          group_by(trial_to_reversal, choice_type, subscale_group) %>%
          summarise(
            mean_accuracy = mean(accuracy),
            se = sd(accuracy) / sqrt(n()),
            .groups = 'drop'
          )
        
        p <- ggplot(summary_data, 
                   aes(x = trial_to_reversal, 
                       y = mean_accuracy, 
                       color = choice_type,
                       group = choice_type)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin = mean_accuracy - se, 
                           ymax = mean_accuracy + se), 
                       width = 0.2) +
          facet_wrap(~subscale_group) +
          scale_color_manual(values = c("Choice 1" = "lightblue", 
                                      "Choice 2" = "darkblue")) +
          labs(x = "Trial relative to reversal",
               y = "Accuracy",
               title = paste("Effect of", subscale, "(Median-split)"),
               subtitle = paste("p =", 
                              format.pval(moderation_results[[scale]][[subscale]]$p_value, 
                                        digits = 3))) +
          theme_custom
        
        print(p)
      }
    }
  }
} else {
  print("No significant results to analyze with moderation")
}

```

Bet magnitude relative to reversal (plus and minus 3 trials)

```{r}
# Read data
df <- read_csv(here("data", "preprocessed", "merged_test_data.csv"), show_col_types = FALSE)

# First, let's see when reversals occur
reversal_trials <- df %>% 
  filter(group.reversal_happened == 1) %>%
  select(group.trial_number) %>%
  distinct()
print("Reversal occurs on trials:")
print(reversal_trials)

# Prepare base questionnaire data first
base_df <- df %>%
  distinct(participant.id_in_session, .keep_all = TRUE) %>%
  select(participant.id_in_session, ssms, dass, lsas, srp_sf, ami, aq_10, age, gender)

# Now create trial-level data and join with questionnaire data
analysis_df <- df %>%
  select(
    participant.id_in_session,
    group.trial_number,
    player.bet1,
    player.bet2,
    group.reversal_happened
  ) %>%
  # Get trials relative to reversal and check for complete windows
  group_by(participant.id_in_session) %>%
  mutate(
    # For each trial, get its position relative to the closest reversal
    trial_to_reversal = map_dbl(group.trial_number, function(x) {
      reversal_trial <- reversal_trials$group.trial_number
      relative_pos <- x - reversal_trial
      if(any(abs(relative_pos) <= 3)) {
        relative_pos[which.min(abs(relative_pos))]
      } else {
        NA
      }
    }),
    # For each trial within a window, check if all 7 trials exist
    complete_window = map_lgl(group.trial_number, function(x) {
      reversal_trial <- reversal_trials$group.trial_number
      rel_pos <- x - reversal_trial
      if(any(abs(rel_pos) <= 3)) {
        closest_reversal <- reversal_trial[which.min(abs(rel_pos))]
        required_trials <- (closest_reversal - 3):(closest_reversal + 3)
        all(required_trials %in% group.trial_number)
      } else {
        FALSE
      }
    })
  ) %>%
  # Keep only complete windows
  filter(!is.na(trial_to_reversal) & complete_window) %>%
  # Reshape to long format
  pivot_longer(
    cols = c(player.bet1, player.bet2),
    names_to = "bet_type",
    values_to = "bet"
  ) %>%
  ungroup() %>%
  # Join with questionnaire data
  left_join(base_df, by = "participant.id_in_session") %>%
  mutate(
    bet_type = factor(bet_type, 
                     levels = c("player.bet1", "player.bet2"),
                     labels = c("Bet 1", "Bet 2")),
    trial_to_reversal = factor(trial_to_reversal),
    participant.id_in_session = factor(participant.id_in_session),
    # Scale questionnaire variables - fixed this line
    across(c(ssms, dass, lsas, srp_sf, ami, aq_10, age), ~scale(.x)[,1])
  )

# Modified model running function
run_questionnaire_models <- function(quest_var, data) {
  print(paste("\nRunning models for:", quest_var))
  
  # Run models
  model1 <- lmer(as.formula(paste("bet ~ bet_type * trial_to_reversal *", 
                                 quest_var, "+ age + (1|participant.id_in_session)")),
                data = data,
                control = lmerControl(optimizer = "bobyqa"))
  
  model2 <- lmer(as.formula(paste("bet ~ bet_type * trial_to_reversal *", 
                                 quest_var, "+ age + (1|participant.id_in_session) + (1|gender)")),
                data = data,
                control = lmerControl(optimizer = "bobyqa"))
  
  # Get best model
  models_list <- list(model1, model2)
  aic_values <- sapply(models_list, AIC)
  winning_model <- models_list[[which.min(aic_values)]]
  
  # Get model statistics
  model_anova <- car::Anova(winning_model, type = 2)
  interaction_p <- model_anova[paste0("bet_type:trial_to_reversal:", quest_var), "Pr(>Chisq)"]
  
  list(
    questionnaire = quest_var,
    winning_model = winning_model,
    p_value = interaction_p,
    aic = min(aic_values)
  )
}

# Custom theme
theme_custom <- theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "right",
    plot.title = element_text(size = 12, face = "bold"),
    plot.subtitle = element_text(size = 10),
    axis.title = element_text(size = 10),
    legend.title = element_text(size = 10),
    legend.text = element_text(size = 9)
  )

# Run analyses
questionnaire_vars <- c("ssms", "dass", "lsas", "srp_sf", "ami", "aq_10")

# Create empty list to store results
all_results <- list()

# Run models for each questionnaire
for(var in questionnaire_vars) {
  result <- run_questionnaire_models(var, analysis_df)
  all_results[[var]] <- tibble(
    questionnaire = result$questionnaire,
    p_value = result$p_value,
    p_adjusted = p.adjust(result$p_value, method = "fdr"),
    aic = result$aic
  )
}

# Combine results into a single dataframe
model_results <- bind_rows(all_results)

# Print results
print("Model results for each questionnaire:")
print(model_results %>% 
        select(questionnaire, p_value, p_adjusted) %>%
        mutate(across(where(is.numeric), ~round(., 3))))

# Create plots for significant relationships
significant_results <- model_results %>%
  filter(p_adjusted < 0.5)

if(nrow(significant_results) > 0) {
  for(i in 1:nrow(significant_results)) {
    var <- significant_results$questionnaire[i]
    
    # Median-split plot
    summary_data <- analysis_df %>%
      mutate(
        quest_group = ifelse(.data[[var]] > median(.data[[var]]), "High", "Low")
      ) %>%
      group_by(trial_to_reversal, bet_type, quest_group) %>%
      summarise(
        mean_bet = mean(bet),
        se = sd(bet) / sqrt(n()),
        .groups = 'drop'
      )
    
    # Create separate plots for High and Low groups
    p <- ggplot(summary_data, 
                aes(x = trial_to_reversal, 
                    y = mean_bet, 
                    color = bet_type,
                    group = bet_type)) +
      geom_line() +
      geom_point() +
      geom_errorbar(aes(ymin = mean_bet - se, 
                        ymax = mean_bet + se), 
                    width = 0.2) +
      facet_wrap(~quest_group) +
      scale_color_manual(values = c("Bet 1" = "#90EE90", "Bet 2" = "#006400")) +
      labs(x = "Trial relative to reversal",
           y = "Bet magnitude",
           title = paste("Effect of", var, "(Median-split)"),
           subtitle = paste("p =", format.pval(significant_results$p_value[i], digits = 3))) +
      theme_custom
    
    print(p)
  }
} else {
  print("No significant relationships found after multiple comparison correction")
}

################## MODERATION FOR SIGNIFICANT RESULTS ONLY ###################

# Define subscale mapping
subscale_mapping <- list(
  lsas = c("lsas_p", "lsas_s"),
  dass = c("dass_a", "dass_d", "dass_s"),
  ssms = c("ssms_cd", "ssms_ia"),
  srp_sf = c("srp_sf_ipm", "srp_sf_ca", "srp_sf_els", "srp_sf_ct"),
  ami = c("ami_es", "ami_sm", "ami_ba"),
  aq_10 = NULL
)

# When extracting subscale data, convert participant ID to factor to match analysis_df
subscale_data <- df %>%
  group_by(participant.id_in_session) %>%
  slice(1) %>%
  select(participant.id_in_session,
         lsas_p, lsas_s,
         dass_a, dass_d, dass_s,
         ssms_cd, ssms_ia,
         srp_sf_ipm, srp_sf_ca, srp_sf_els, srp_sf_ct,
         ami_es, ami_sm, ami_ba) 

# Scale the subscales and convert participant ID to factor
subscale_data_scaled <- subscale_data %>%
  ungroup() %>%
  mutate(
    across(-participant.id_in_session, ~as.vector(scale(.x))),
    participant.id_in_session = factor(participant.id_in_session)
  )

# Define function for running moderation analysis
run_subscale_moderation <- function(scale_name, analysis_df, subscales) {
  results <- list()
  
  for(subscale in subscales) {
    print(paste("\nRunning moderation analysis for subscale:", subscale))
    
    formula_str <- paste0("bet ~ bet_type * trial_to_reversal * ", subscale, 
                         " + age + (1|participant.id_in_session)")
    
    print(paste("Trying to run model with formula:", formula_str))
    
    model <- lmer(as.formula(formula_str),
                 data = analysis_df,
                 control = lmerControl(optimizer = "bobyqa"))
    
    # Get model statistics
    model_anova <- car::Anova(model, type = 2)
    interaction_term <- paste0("bet_type:trial_to_reversal:", subscale)
    interaction_p <- model_anova[interaction_term, "Pr(>Chisq)"]
    
    results[[subscale]] <- list(
      subscale = subscale,
      model = model,
      p_value = interaction_p
    )
  }
  
  return(results)
}

# Join scaled subscales to analysis_df
analysis_df_with_subscales <- analysis_df %>%
  left_join(subscale_data_scaled, by = "participant.id_in_session")

# Run moderation for significant results
significant_scales <- model_results %>%
  filter(p_adjusted < 0.5) %>%
  pull(questionnaire)

if(length(significant_scales) > 0) {
  moderation_results <- list()
  
  for(scale in significant_scales) {
    subscales <- subscale_mapping[[scale]]
    if(!is.null(subscales)) {
      moderation_results[[scale]] <- run_subscale_moderation(
        scale, 
        analysis_df_with_subscales, 
        subscales
      )
      
      # Print results
      cat("\nModeration Results for", scale, "subscales:\n")
      subscale_summary <- map_df(moderation_results[[scale]], function(result) {
        tibble(
          subscale = result$subscale,
          p_value = result$p_value,
          p_adjusted = p.adjust(result$p_value, method = "fdr")
        )
      })
      print(subscale_summary)
      
      # Create plots for significant subscale moderations
      significant_subscales <- subscale_summary %>%
        filter(p_adjusted < 0.5) %>%
        pull(subscale)
      
      for(subscale in significant_subscales) {
        summary_data <- analysis_df_with_subscales %>%
          mutate(
            subscale_group = ifelse(.data[[subscale]] > median(.data[[subscale]]), 
                                  "High", "Low")
          ) %>%
          group_by(trial_to_reversal, bet_type, subscale_group) %>%
          summarise(
            mean_bet = mean(bet),
            se = sd(bet) / sqrt(n()),
            .groups = 'drop'
          )
        
        p <- ggplot(summary_data, 
                   aes(x = trial_to_reversal, 
                       y = mean_bet, 
                       color = bet_type,
                       group = bet_type)) +
          geom_line() +
          geom_point() +
          geom_errorbar(aes(ymin = mean_bet - se, 
                           ymax = mean_bet + se), 
                       width = 0.2) +
          facet_wrap(~subscale_group) +
          scale_color_manual(values = c("Bet 1" = "#90EE90", 
                                      "Bet 2" = "#006400")) +
          labs(x = "Trial relative to reversal",
               y = "Bet magnitude",
               title = paste("Effect of", subscale, "(Median-split)"),
               subtitle = paste("p =", 
                              format.pval(moderation_results[[scale]][[subscale]]$p_value, 
                                        digits = 3))) +
          theme_custom
        
        print(p)
      }
    }
  }
} else {
  print("No significant results to analyze with moderation")
}

```





